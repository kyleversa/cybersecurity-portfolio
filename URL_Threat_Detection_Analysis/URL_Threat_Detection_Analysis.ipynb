{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a71b055",
   "metadata": {},
   "source": [
    "# URL Threat Detection & Security Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76725381",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This security research project analyzes honeypot data to identify and classify malicious URL-based attacks. By detecting patterns in URLs associated with suspicious activities, this analysis helps identify potential cyber threats and vulnerabilities within systems.\n",
    "\n",
    "Understanding attack types and patterns enables organizations to better secure their networks, protect sensitive data, and ensure the integrity and availability of their services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077df57",
   "metadata": {},
   "source": [
    "### Start Program Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3d92c",
   "metadata": {},
   "source": [
    "#### Libraries used\n",
    "\n",
    "This section loads all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ec30ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "URL Threat Detection & Security Analysis\n",
      "============================================================\n",
      "Python version: 3.13.5\n",
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.1.3\n",
      "Execution time: 2026-02-14 13:33:53\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#### Libraries used\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "# Using ThreadPool (threading) instead of multiprocessing for better Jupyter compatibility\n",
    "# Threads share memory space, avoiding serialization overhead in notebook environments\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from typing import Dict, List, Tuple, Set, Optional, Any\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import display\n",
    "\n",
    "# Configure environment\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Version information for reproducibility\n",
    "print(\"=\"*60)\n",
    "print(\"URL Threat Detection & Security Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python version: {os.sys.version.split()[0]}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Execution time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b737a",
   "metadata": {},
   "source": [
    "#### Set Data Path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ebc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Set Data Path:\n",
    "# Configure working directories and create output folder for analysis results\n",
    "\n",
    "# Set paths \n",
    "main_dir = os.getcwd()\n",
    "data_dir = main_dir\n",
    "output_dir = os.path.join(main_dir, 'Reports')  # Save outputs to a subfolder\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Remove annoying error and improve display\n",
    "pd.options.mode.chained_assignment = None \n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc15bc9c",
   "metadata": {},
   "source": [
    "#### Regular Expressions (regex):\n",
    "\n",
    "This section contains the regex statements that will be used to locate suspicious activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfac8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing optimized attack detection system...\n",
      "Optimized attack patterns loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#### Regular Expressions (regex):\n",
    "# This section contains the regex statements that will be used to locate suspicious activity\n",
    "\n",
    "class OptimizedAttackPatterns:\n",
    "    \"\"\"\n",
    "    High-performance modular attack pattern management system.\n",
    "    \n",
    "    PERFORMANCE OPTIMIZATIONS IMPLEMENTED:\n",
    "    - Pre-filtering with fast string operations\n",
    "    - Simplified regex patterns to reduce backtracking\n",
    "    - Pattern caching with LRU cache\n",
    "    - Tiered detection \n",
    "    - Non-capturing groups and possessive quantifiers\n",
    "    - Word boundaries for better anchoring\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-filter keywords for fast initial screening\n",
    "    _SQL_KEYWORDS = {'union', 'select', 'insert', 'update', 'delete', 'drop', 'create', 'alter', 'exec', 'waitfor', 'sleep'}\n",
    "    _XSS_KEYWORDS = {'script', 'javascript', 'onload', 'onerror', 'onclick', 'alert', 'eval', 'document', 'window'}\n",
    "    _PHP_KEYWORDS = {'eval', 'exec', 'system', 'shell_exec', 'base64_decode', 'gzinflate', 'file_get_contents', 'include', 'require'}\n",
    "    _CMD_KEYWORDS = {'cmd', 'powershell', 'bash', 'sh', 'wget', 'curl', 'nc', 'netcat', 'ping', 'tracert'}\n",
    "    _TRAVERSAL_KEYWORDS = {'../', '..\\\\', '%2e%2e', 'etc/passwd', 'windows/system32', 'proc/self'}\n",
    "    \n",
    "    @staticmethod\n",
    "    def quick_prefilter(text: str, keywords: Set[str]) -> bool:\n",
    "        \"\"\"Fast string-based pre-filtering before regex matching.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        return any(keyword in text_lower for keyword in keywords)\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=32)\n",
    "    def get_sql_injection_pattern() -> re.Pattern:\n",
    "        \"\"\"\n",
    "        Optimized SQL Injection detection pattern.\n",
    "        \n",
    "        OPTIMIZATIONS:\n",
    "        - Removed nested quantifiers that cause backtracking\n",
    "        - Used word boundaries for better performance\n",
    "        - Simplified encoding patterns\n",
    "        - Combined similar patterns\n",
    "        - Used non-capturing groups\n",
    "        \n",
    "        Returns:\n",
    "            re.Pattern: Compiled regex pattern for SQL injection detection\n",
    "        \"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            # Core SQL injection - most common first\n",
    "            r\"\\bunion\\s+(?:all\\s+)?select\\b|\"             # UNION SELECT (most common)\n",
    "            r\"\\bselect\\s+[\\w*,\\s]+\\bfrom\\b|\"              # SELECT FROM statements\n",
    "            r\"\\b(?:insert|update|delete)\\s+(?:into\\s+)?\\w+|\"  # DML operations\n",
    "            r\"\\bdrop\\s+(?:table|database|schema)\\b|\"      # DROP operations\n",
    "            \n",
    "            # SQL functions and techniques\n",
    "            r\"\\b(?:exec|execute)(?:\\s*\\(|\\s+)|\"           # EXEC/EXECUTE\n",
    "            r\"\\bwaitfor\\s+delay\\b|\"                       # Time delays\n",
    "            r\"\\bsleep\\s*\\([^)]*\\)|\"                       # SLEEP function\n",
    "            r\"\\bbenchmark\\s*\\([^)]*\\)|\"                   # BENCHMARK\n",
    "            \n",
    "            # Boolean-based blind injection \n",
    "            r\"\\b(?:and|or)\\s+\\d+\\s*[=<>!]+\\s*\\d+|\"      # Numeric comparisons\n",
    "            r\"\\b(?:and|or)\\s+[\\w'\\\"]+\\s*=\\s*[\\w'\\\"]+|\"   # String comparisons\n",
    "            \n",
    "            # SQL comments and terminators\n",
    "            r\"--[^\\r\\n]*|\"                               # Line comments\n",
    "            r\"/\\*.*?\\*/|\"                                 # Block comments\n",
    "            r\"[;\\x00]|\"                                   # Statement terminators\n",
    "            \n",
    "            # Database metadata \n",
    "            r\"\\binformation_schema\\b|\"                    # Info schema\n",
    "            r\"\\b(?:sys|mysql)\\.(?:tables|users?|columns?)\\b|\"  # System tables\n",
    "            \n",
    "            # Common encoding (most frequent only)\n",
    "            r\"%27|%22|%20union%20|%20select%20|\"         # URL encoded\n",
    "            r\"0x[0-9a-f]+|\"                              # Hex values\n",
    "            \n",
    "            # Database functions (critical ones only)\n",
    "            r\"\\b(?:concat|substring|ascii|char)\\s*\\(|\"    # String functions\n",
    "            r\"\\b(?:user|database|version)\\s*\\(\\s*\\)|\"     # Info functions\n",
    "            \n",
    "            # Extra techniques \n",
    "            r\"\\binto\\s+(?:outfile|dumpfile)\\b|\"          # File operations\n",
    "            r\"@@(?:version|user|hostname)|\"               # MySQL variables\n",
    "            r\"\\bxp_cmdshell\\b\"                           # MSSQL command shell\n",
    "            r\")\",\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=32)\n",
    "    def get_xss_pattern() -> re.Pattern:\n",
    "        \"\"\"\n",
    "        Optimized Cross-Site Scripting (XSS) detection pattern.\n",
    "        \n",
    "        OPTIMIZATIONS:\n",
    "        - Focused on most dangerous XSS vectors\n",
    "        - Simplified HTML tag matching\n",
    "        - Reduced complex nested groups\n",
    "        - Combined encoding patterns\n",
    "        \n",
    "        Returns:\n",
    "            re.Pattern: Compiled regex pattern for XSS detection\n",
    "        \"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            # Script tags (most critical)\n",
    "            r\"<script[^>]*>|</script>|\"                   # Script tags\n",
    "            r\"%3Cscript|%3C/script|\"                      # URL encoded\n",
    "            r\"&lt;script|&lt;/script|\"                    # HTML encoded\n",
    "            \n",
    "            # JavaScript protocols and events\n",
    "            r\"javascript\\s*:|\"                           # JavaScript protocol\n",
    "            r\"\\bon\\w+\\s*=|\"                              # Event handlers\n",
    "            r\"\\b(?:alert|prompt|confirm)\\s*\\(|\"          # Dialog functions\n",
    "            \n",
    "            # Dangerous HTML tags \n",
    "            r\"<(?:iframe|embed|object|applet|meta|link)\\b[^>]*>|\"  # Dangerous tags\n",
    "            r\"<(?:img|body)\\b[^>]*\\bon\\w+[^>]*>|\"        # Tags with events\n",
    "            \n",
    "            # DOM manipulation\n",
    "            r\"\\b(?:document|window|self|top|parent)\\.|\\binnerHTML\\b|\"  # DOM access\n",
    "            r\"\\.(?:cookie|location|href)\\b|\"              # Sensitive properties\n",
    "            \n",
    "            # Common encoding patterns \n",
    "            r\"&#(?:\\d+|x[0-9a-f]+);|\"                   # HTML entities\n",
    "            r\"%u[0-9a-f]{4}|\"                            # Unicode encoding\n",
    "            r\"String\\.fromCharCode|\"                      # Char code conversion\n",
    "            \n",
    "            # Data URIs and special content\n",
    "            r\"data:(?:text/html|application/x-javascript)|\"  # Data URIs\n",
    "            r\"<!\\[CDATA\\[|\"                              # CDATA sections\n",
    "            r\"\\beval\\s*\\(|setTimeout\\s*\\(\"               # Code execution\n",
    "            r\")\",\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=32)\n",
    "    def get_php_attack_pattern() -> re.Pattern:\n",
    "        \"\"\"\n",
    "        Optimized PHP attack detection pattern.\n",
    "        \n",
    "        OPTIMIZATIONS:\n",
    "        - Focused on most dangerous PHP functions\n",
    "        - Simplified file extension matching\n",
    "        - Reduced redundant shell name patterns\n",
    "        - Combined encoding functions\n",
    "        \n",
    "        Returns:\n",
    "            re.Pattern: Compiled regex pattern for PHP attack detection\n",
    "        \"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            # PHP file indicators\n",
    "            r\"\\.php[345]?\\b|\\.phtml\\b|\"                   # PHP extensions\n",
    "            \n",
    "            # Critical PHP shells (most common)\n",
    "            r\"\\b(?:c99|r57|wso|b374k|shell|cmd|evil|bypass)\\.php\\b|\"  # Common shells\n",
    "            \n",
    "            # Dangerous PHP functions (highest priority)\n",
    "            r\"\\b(?:eval|assert|exec|system|shell_exec|passthru)\\s*\\(|\"  # Code execution\n",
    "            r\"\\b(?:file_get_contents|file_put_contents|fopen|fwrite)\\s*\\(|\"  # File operations\n",
    "            r\"\\bcreate_function\\s*\\(|\"                    # Dynamic functions\n",
    "            r\"\\bpreg_replace\\s*\\([^)]*[\\\"']/e[\\\"']|\"     # Regex with eval\n",
    "            \n",
    "            # Critical encoding/decoding \n",
    "            r\"\\b(?:base64_decode|gzinflate|str_rot13)\\s*\\(|\"  # Decoding\n",
    "            r\"\\b(?:gzuncompress|convert_uudecode)\\s*\\(|\"  # Decompression\n",
    "            \n",
    "            # System commands\n",
    "            r\"\\bproc_open\\s*\\(|\"                          # Process control\n",
    "            r\"\\b(?:popen|fsockopen|socket_create)\\s*\\(|\"  # Network/process\n",
    "            \n",
    "            # Include/require\n",
    "            r\"\\b(?:include|require)(?:_once)?\\s*\\(?|\"     # File inclusion\n",
    "            \n",
    "            # PHP superglobals \n",
    "            r\"\\$_(?:GET|POST|REQUEST|FILES|COOKIE)\\s*\\[|\"  # Input arrays\n",
    "            \n",
    "            # PHP information\n",
    "            r\"\\b(?:phpinfo|php_uname|system)\\s*\\(|\"      # Info disclosure\n",
    "            \n",
    "            # Common database functions \n",
    "            r\"\\b(?:mysql_|mysqli_)(?:connect|query)\\s*\\(|\"  # MySQL functions\n",
    "            \n",
    "            # Stream wrappers\n",
    "            r\"\\bphp://(?:input|filter|fd)|\"               # PHP wrappers\n",
    "            r\"\\b(?:file|data|zip)://|\"                    # File wrappers\n",
    "            \n",
    "            # Serialization\n",
    "            r\"\\b(?:unserialize|__wakeup|__destruct)\\s*\\(|\"  # Serialization\n",
    "            \n",
    "            # Known vulnerable components\n",
    "            r\"\\b(?:timthumb|uploadify|adminer)\\.php\\b\"    # Vulnerable files\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=32)\n",
    "    def get_windows_attack_pattern() -> re.Pattern:\n",
    "        \"\"\"\n",
    "        Optimized Windows attack detection pattern.\n",
    "        \n",
    "        OPTIMIZATIONS:\n",
    "        - Focused on most critical Windows attack vectors\n",
    "        - Simplified path matching\n",
    "        - Combined registry patterns\n",
    "        - Reduced PowerShell pattern complexity\n",
    "        \n",
    "        Returns:\n",
    "            re.Pattern: Compiled regex pattern for Windows attack detection\n",
    "        \"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            # Directory traversal (Windows)\n",
    "            r\"\\.\\.\\\\|%5c\\.\\.%5c|%c1%9c|\"                 # Path traversal\n",
    "            \n",
    "            # Critical Windows executables \n",
    "            r\"\\b(?:cmd|powershell|wscript|cscript|mshta)\\.exe\\b|\"  # Executables\n",
    "            r\"\\b(?:regsvr32|rundll32|certutil|bitsadmin)\\.exe\\b|\"  # System tools\n",
    "            \n",
    "            # Windows paths \n",
    "            r\"[c-z]:\\\\\\\\|\\\\\\\\windows\\\\\\\\|\\\\\\\\system32\\\\\\\\|\"  # System paths\n",
    "            r\"%(?:windir|systemroot|temp)%|\"              # Environment vars\n",
    "            \n",
    "            # Dangerous Windows commands \n",
    "            r\"\\bnet\\s+(?:user|localgroup|share)\\b|\"       # Net commands\n",
    "            r\"\\b(?:sc|reg)\\s+(?:create|add|delete)\\b|\"    # Service/registry\n",
    "            r\"\\bwmic\\s+(?:process|service)\\b|\"            # WMI commands\n",
    "            \n",
    "            # PowerShell \n",
    "            r\"-EncodedCommand\\b|\"                         # Encoded PS\n",
    "            r\"\\b(?:Invoke-Expression|Invoke-Command)\\b|\"  # PS execution\n",
    "            r\"\\bDownloadString\\b|Net\\.WebClient\\b|\"       # PS download\n",
    "            r\"-ExecutionPolicy\\s+Bypass|\"                 # PS policy bypass\n",
    "            \n",
    "            # Critical registry paths \n",
    "            r\"\\bHKEY_(?:LOCAL_MACHINE|CURRENT_USER)\\b|\"   # Registry hives\n",
    "            r\"\\\\CurrentVersion\\\\Run\\b|\"                   # Autorun keys\n",
    "            \n",
    "            # High-risk Windows files \n",
    "            r\"\\b(?:boot|win|system)\\.ini\\b|\"              # System files\n",
    "            r\"\\b(?:sam|security|software)\\b|\"             # Registry files\n",
    "            \n",
    "            # ASP/ASPX\n",
    "            r\"\\.asp[x]?\\b|\"                               # ASP files\n",
    "            r\"\\b(?:eval|execute)\\s+request\\b|\"            # ASP execution\n",
    "            \n",
    "            # Dangerous file extensions \n",
    "            r\"\\.(?:bat|cmd|vbs|ps1|scr|hta)\\b|\"          # Executable extensions\n",
    "            \n",
    "            # Common Windows malware/tools \n",
    "            r\"\\b(?:mimikatz|psexec|procdump|wmiexec)\\b|\"  # Attack tools\n",
    "            \n",
    "            # Privilege escalation\n",
    "            r\"\\b(?:sudo|runas|gsudo)\\s+|\"                 # Elevation\n",
    "            r\"\\bschtasks\\s*/create\\b\"                     # Scheduled tasks\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=32)\n",
    "    def get_directory_traversal_pattern() -> re.Pattern:\n",
    "        \"\"\"\n",
    "        Optimized Directory Traversal detection pattern.\n",
    "        \n",
    "        OPTIMIZATIONS:\n",
    "        - Focused on most common traversal patterns\n",
    "        - Simplified encoding variations\n",
    "        - Combined similar file access patterns\n",
    "        \n",
    "        Returns:\n",
    "            re.Pattern: Compiled regex pattern for directory traversal detection\n",
    "        \"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            # Basic traversal patterns\n",
    "            r\"\\.\\.[\\\\/]|\"                                 # Standard traversal\n",
    "            r\"%2e%2e%2f|%2e%2e%5c|\"                      # URL encoded\n",
    "            r\"%252e%252e%252f|%252e%252e%255c|\"          # Double encoded\n",
    "            r\"\\x00|\\%00|\"                                # Null bytes\n",
    "            \n",
    "            # Critical Unix sensitive files \n",
    "            r\"/etc/(?:passwd|shadow|hosts|sudoers)\\b|\"    # System files\n",
    "            r\"/proc/(?:self|version|cmdline)/|\"           # Process info\n",
    "            r\"/var/log/\\w+|\"                             # Log files\n",
    "            \n",
    "            # Web server files\n",
    "            r\"\\.ht(?:access|passwd)\\b|\"                   # Apache files\n",
    "            r\"(?:httpd|nginx)\\.conf\\b|\"                   # Config files\n",
    "            r\"web\\.config\\b|\"                             # IIS config\n",
    "            \n",
    "            # Common application configs \n",
    "            r\"\\b(?:wp-config|configuration|settings)\\.php\\b|\"  # CMS configs\n",
    "            r\"\\.(?:env|git|ssh)/|\"                        # Hidden dirs\n",
    "            r\"\\b(?:id_rsa|authorized_keys)\\b|\"            # SSH keys\n",
    "            \n",
    "            # Database files\n",
    "            r\"\\.(?:sqlite|db|sql|dump)\\b|\"               # Database files\n",
    "            \n",
    "            # Backup and temp files\n",
    "            r\"\\.(?:bak|backup|old|tmp|swp)\\b|\"           # Backup files\n",
    "            \n",
    "            # Archive files\n",
    "            r\"\\.(?:tar|zip|rar|7z)(?:\\.gz)?\\b|\"         # Archives\n",
    "            \n",
    "            # Special protocols\n",
    "            r\"\\b(?:file|php|data|zip)://|\"               # URI schemes\n",
    "            \n",
    "            # Critical Windows specific paths\n",
    "            r\"\\\\windows\\\\system32\\\\|\"                     # System32\n",
    "            r\"\\\\boot\\.ini\\b|\\\\sam\\b\"                     # Windows files\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=32)\n",
    "    def get_command_injection_pattern() -> re.Pattern:\n",
    "        \"\"\"\n",
    "        Optimized Command Injection detection pattern.\n",
    "        \n",
    "        OPTIMIZATIONS:\n",
    "        - Focused on most dangerous command injection vectors\n",
    "        - Simplified shell metacharacter detection\n",
    "        - Combined similar command patterns\n",
    "        \n",
    "        Returns:\n",
    "            re.Pattern: Compiled regex pattern for command injection detection\n",
    "        \"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            # Critical command separators \n",
    "            r\"[;&|`]|&&|\\|\\||\"                           # Shell operators\n",
    "            r\"\\$\\(|\\${|\"                                 # Command substitution\n",
    "            r\">>|2>&1|\"                                  # Redirections\n",
    "            \n",
    "            # Critical system commands\n",
    "            r\"\\b(?:cat|ls|pwd|id|whoami|uname|ps|kill)\\s|\"  # Info commands\n",
    "            r\"\\b(?:wget|curl|nc|netcat|telnet|ssh)\\s|\"   # Network commands\n",
    "            r\"\\b(?:rm|rmdir|mkdir|cp|mv|chmod|chown)\\s|\"  # File commands\n",
    "            r\"\\b(?:bash|sh|zsh|cmd|powershell)\\s|\"       # Shells\n",
    "            \n",
    "            # Dangerous patterns\n",
    "            r\"\\b(?:eval|exec|system)\\s*\\(|\"              # Execution functions\n",
    "            r\"/bin/(?:bash|sh)\\s+-[ci]|\"                 # Interactive shells\n",
    "            r\"mkfifo\\s+|nc\\s+-e|\"                       # Backdoor patterns\n",
    "            \n",
    "            # Environment variables\n",
    "            r\"\\$(?:PATH|HOME|USER|SHELL)\\b|\"             # Environment vars\n",
    "            \n",
    "            # Common encoded patterns \n",
    "            r\"%3B|%7C|%26|%60|\"                          # URL encoded\n",
    "            r\"\\\\x[2-7][0-9a-f]|\"                         # Hex encoded\n",
    "            \n",
    "            # Package managers and interpreters\n",
    "            r\"\\b(?:apt|yum|pip|npm|wget|curl)\\s+\\w+|\"    # Package operations\n",
    "            r\"\\b(?:python|perl|ruby|php|node)\\s+|\"       # Interpreters\n",
    "            \n",
    "            # Time-based and sleep\n",
    "            r\"\\bsleep\\s+\\d+|timeout\\s+\\d+|\"             # Delays\n",
    "            \n",
    "            # File operations\n",
    "            r\">/dev/null|\"                               # Output redirect\n",
    "            r\"/dev/(?:tcp|udp)/|\"                        # Network devices\n",
    "            \n",
    "            # Reverse shells \n",
    "            r\"bash\\s+-i|/bin/sh\\s+-i|\"                   # Interactive\n",
    "            r\"openssl\\s+s_client\"                        # SSL shells\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=16)\n",
    "    def get_ldap_injection_pattern() -> re.Pattern:\n",
    "        \"\"\"Optimized LDAP Injection detection pattern.\"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            r\"\\(\\w*=\\*\\)|\"                               # LDAP wildcards\n",
    "            r\"[|&!]\\(|\"                                  # LDAP operators\n",
    "            r\"\\b(?:cn|uid|mail|objectClass)=|\"           # Common attributes\n",
    "            r\"%28|%29|%2A|%7C|%26\"                       # Encoded chars\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=16)\n",
    "    def get_xml_injection_pattern() -> re.Pattern:\n",
    "        \"\"\"Optimized XML Injection detection pattern.\"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            r\"<!(?:ENTITY|DOCTYPE|ELEMENT|ATTLIST)\\b|\"   # XML declarations\n",
    "            r\"<!\\[CDATA\\[|\"                              # CDATA\n",
    "            r\"\\b(?:SYSTEM|PUBLIC)\\s+[\\\"']|\"              # External entities\n",
    "            r\"xmlns:|&\\w+;|\"                             # Namespaces/entities\n",
    "            r\"%3C!(?:ENTITY|DOCTYPE)\"                    # Encoded\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=16)\n",
    "    def get_nosql_injection_pattern() -> re.Pattern:\n",
    "        \"\"\"Optimized NoSQL Injection detection pattern.\"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            r\"\\$(?:ne|eq|gt|gte|lt|lte|in|nin|and|or|not|nor)\\b|\"  # Operators\n",
    "            r\"\\$(?:exists|type|all|size|regex|where|elemMatch)\\b|\"  # Query ops\n",
    "            r\"[\\{\\[]?\\$\\w+|\"                             # Object notation\n",
    "            r\"\\bfunction\\s*\\(|this\\.|\"                   # JavaScript\n",
    "            r\"return\\s+\\w+\"                              # Return statements\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=16)\n",
    "    def get_file_inclusion_pattern() -> re.Pattern:\n",
    "        \"\"\"Optimized File Inclusion detection pattern.\"\"\"\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:\"\n",
    "            # Remote inclusion\n",
    "            r\"(?:https?|ftp|php|data|glob|phar)://|\"     # Remote protocols\n",
    "            # Local inclusion\n",
    "            r\"\\.\\.[\\\\/]\\.\\.[\\\\/]|\"                       # Multiple traversal\n",
    "            r\"\\?(?:file|path|page|include|require)=|\"    # Inclusion params\n",
    "            r\"%00|\\\\x00\"                                 # Null bytes\n",
    "            r\")\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        return pattern\n",
    "\n",
    "\n",
    "class PerformanceAnalyzer:\n",
    "    \"\"\"\n",
    "    High-performance pattern matching with tiered detection system.\n",
    "    \n",
    "    PERFORMANCE FEATURES:\n",
    "    - Pre-filtering eliminates some non-malicious requests\n",
    "    - Pattern caching reduces compilation overhead\n",
    "    - Tiered matching \n",
    "    - Batch processing capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = OptimizedAttackPatterns()\n",
    "        self.stats = {\n",
    "            'total_processed': 0,\n",
    "            'prefilter_passed': 0,\n",
    "            'regex_matches': 0,\n",
    "            'pattern_cache_hits': 0\n",
    "        }\n",
    "    \n",
    "    def analyze_single(self, text: str) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Analyze a single text input for multiple attack types.\n",
    "        \n",
    "        Returns:\n",
    "            Dict mapping attack types to boolean detection results\n",
    "        \"\"\"\n",
    "        self.stats['total_processed'] += 1\n",
    "        results = {}\n",
    "        \n",
    "        # SQL Injection\n",
    "        if OptimizedAttackPatterns.quick_prefilter(text, OptimizedAttackPatterns._SQL_KEYWORDS):\n",
    "            self.stats['prefilter_passed'] += 1\n",
    "            results['sql_injection'] = bool(self.patterns.get_sql_injection_pattern().search(text))\n",
    "        else:\n",
    "            results['sql_injection'] = False\n",
    "        \n",
    "        # XSS\n",
    "        if OptimizedAttackPatterns.quick_prefilter(text, OptimizedAttackPatterns._XSS_KEYWORDS):\n",
    "            results['xss'] = bool(self.patterns.get_xss_pattern().search(text))\n",
    "        else:\n",
    "            results['xss'] = False\n",
    "        \n",
    "        # PHP Attacks\n",
    "        if OptimizedAttackPatterns.quick_prefilter(text, OptimizedAttackPatterns._PHP_KEYWORDS):\n",
    "            results['php_attack'] = bool(self.patterns.get_php_attack_pattern().search(text))\n",
    "        else:\n",
    "            results['php_attack'] = False\n",
    "        \n",
    "        # Windows Attacks\n",
    "        if OptimizedAttackPatterns.quick_prefilter(text, OptimizedAttackPatterns._CMD_KEYWORDS):\n",
    "            results['windows_attack'] = bool(self.patterns.get_windows_attack_pattern().search(text))\n",
    "        else:\n",
    "            results['windows_attack'] = False\n",
    "        \n",
    "        # Directory Traversal\n",
    "        if OptimizedAttackPatterns.quick_prefilter(text, OptimizedAttackPatterns._TRAVERSAL_KEYWORDS):\n",
    "            results['directory_traversal'] = bool(self.patterns.get_directory_traversal_pattern().search(text))\n",
    "        else:\n",
    "            results['directory_traversal'] = False\n",
    "        \n",
    "        # Command Injection\n",
    "        if OptimizedAttackPatterns.quick_prefilter(text, OptimizedAttackPatterns._CMD_KEYWORDS):\n",
    "            results['command_injection'] = bool(self.patterns.get_command_injection_pattern().search(text))\n",
    "        else:\n",
    "            results['command_injection'] = False\n",
    "        \n",
    "        # Other patterns \n",
    "        results['ldap_injection'] = bool(self.patterns.get_ldap_injection_pattern().search(text))\n",
    "        results['xml_injection'] = bool(self.patterns.get_xml_injection_pattern().search(text))\n",
    "        results['nosql_injection'] = bool(self.patterns.get_nosql_injection_pattern().search(text))\n",
    "        results['file_inclusion'] = bool(self.patterns.get_file_inclusion_pattern().search(text))\n",
    "        \n",
    "        # Count matches\n",
    "        if any(results.values()):\n",
    "            self.stats['regex_matches'] += 1\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_batch(self, texts: List[str]) -> List[Dict[str, bool]]:\n",
    "        \"\"\"\n",
    "        Analyze multiple texts in batch for better performance.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to analyze\n",
    "            \n",
    "        Returns:\n",
    "            List of detection results for each input\n",
    "        \"\"\"\n",
    "        return [self.analyze_single(text) for text in texts]\n",
    "    \n",
    "    def get_performance_stats(self) -> Dict:\n",
    "        \"\"\"Get performance statistics.\"\"\"\n",
    "        if self.stats['total_processed'] > 0:\n",
    "            prefilter_efficiency = (1 - self.stats['prefilter_passed'] / self.stats['total_processed']) * 100\n",
    "            match_rate = (self.stats['regex_matches'] / self.stats['total_processed']) * 100\n",
    "        else:\n",
    "            prefilter_efficiency = 0\n",
    "            match_rate = 0\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            'prefilter_efficiency_percent': round(prefilter_efficiency, 2),\n",
    "            'match_rate_percent': round(match_rate, 2)\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear pattern cache for memory management.\"\"\"\n",
    "        # Clear the LRU caches\n",
    "        OptimizedAttackPatterns.get_sql_injection_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_xss_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_php_attack_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_windows_attack_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_directory_traversal_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_command_injection_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_ldap_injection_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_xml_injection_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_nosql_injection_pattern.cache_clear()\n",
    "        OptimizedAttackPatterns.get_file_inclusion_pattern.cache_clear()\n",
    "\n",
    "\n",
    "# Initialize the performance analyzer\n",
    "print(\"Initializing optimized attack detection system...\")\n",
    "analyzer = PerformanceAnalyzer()\n",
    "\n",
    "print(\"Optimized attack patterns loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55007071",
   "metadata": {},
   "source": [
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6c5284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functions loaded successfully!\n",
      "System ready for security analysis\n"
     ]
    }
   ],
   "source": [
    "#### Functions:\n",
    "# Core analysis classes and utility functions for URL security detection\n",
    "\n",
    "class VectorizedURLAnalyzer:\n",
    "    \"\"\"\n",
    "    URL Security Analyzer using vectorized operations and parallel processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patterns: Dict[str, re.Pattern]):\n",
    "        \"\"\"\n",
    "        Initialize with pre-compiled patterns.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        patterns : Dict[str, re.Pattern]\n",
    "            Dictionary of attack names to compiled regex patterns\n",
    "        \"\"\"\n",
    "        self.patterns = patterns\n",
    "        self.severity_weights = {\n",
    "            'SQL Injection': 10,\n",
    "            'Command Injection': 10,\n",
    "            'File Inclusion': 9,\n",
    "            'Directory Traversal': 8,\n",
    "            'Cross-Site Scripting (XSS)': 7,\n",
    "            'PHP Attack': 7,\n",
    "            'Windows Attack': 6,\n",
    "            'XML Injection': 5,\n",
    "            'LDAP Injection': 5,\n",
    "            'NoSQL Injection': 5\n",
    "        }\n",
    "\n",
    "    def analyze_single_url(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze a single URL for attack patterns.\"\"\"\n",
    "        if pd.isna(url) or url == '':\n",
    "            url = '/EMPTY_URL'\n",
    "        \n",
    "        url = str(url)\n",
    "        matched_attacks = []\n",
    "        \n",
    "        # Check each pattern\n",
    "        for attack_type, pattern in self.patterns.items():\n",
    "            try:\n",
    "                if pattern.search(url):\n",
    "                    matched_attacks.append(attack_type)\n",
    "            except Exception:\n",
    "                # Skip problematic URLs\n",
    "                continue\n",
    "        \n",
    "        # Calculate results\n",
    "        is_suspicious = bool(matched_attacks)\n",
    "        attack_count = len(matched_attacks)\n",
    "        severity_score = sum([self.severity_weights.get(attack, 1) for attack in matched_attacks])\n",
    "        \n",
    "        # Determine severity level\n",
    "        if severity_score >= 20:\n",
    "            severity = 'Critical'\n",
    "        elif severity_score >= 15:\n",
    "            severity = 'High'\n",
    "        elif severity_score >= 8:\n",
    "            severity = 'Medium'\n",
    "        elif severity_score > 0:\n",
    "            severity = 'Low'\n",
    "        else:\n",
    "            severity = 'Clean'\n",
    "        \n",
    "        # Calculate confidence \n",
    "        confidence = min(1.0, attack_count * 0.3) if is_suspicious else 0.0\n",
    "        \n",
    "        return {\n",
    "            'is_suspicious': is_suspicious,\n",
    "            'attack_types': matched_attacks,\n",
    "            'attack_count': attack_count,\n",
    "            'severity': severity,\n",
    "            'severity_score': severity_score,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "    def batch_analyze_chunk(self, urls_chunk: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Analyze a chunk of URLs.\"\"\"\n",
    "        results = []\n",
    "        for url in urls_chunk:\n",
    "            result = self.analyze_single_url(url)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def analyze_batch_parallel(self, urls: List[str], n_jobs: int = 4) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Analyze URLs in parallel using thread-safe approach.\n",
    "        \"\"\"\n",
    "        print(f\"Analyzing {len(urls):,} URLs using {n_jobs} threads...\")\n",
    "\n",
    "        # Using ThreadPool (threading) instead of multiprocessing for better Jupyter compatibility\n",
    "        # Threads share memory space, avoiding serialization overhead in notebook environments\n",
    "        \n",
    "        # Create chunks\n",
    "        chunk_size = max(100, len(urls) // n_jobs)  # Minimum 100 URLs per chunk\n",
    "        chunks = [urls[i:i + chunk_size] for i in range(0, len(urls), chunk_size)]\n",
    "        \n",
    "        print(f\"Created {len(chunks)} chunks of ~{chunk_size} URLs each\")\n",
    "        \n",
    "        # Process chunks with thread pool\n",
    "        results = []\n",
    "        with ThreadPool(n_jobs) as pool:\n",
    "            chunk_results = pool.map(self.batch_analyze_chunk, chunks)\n",
    "        \n",
    "        # Flatten results\n",
    "        for chunk_result in chunk_results:\n",
    "            results.extend(chunk_result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_summary_stats(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate summary statistics from results.\"\"\"\n",
    "        if not results:\n",
    "            return {'total_urls': 0, 'suspicious_urls': 0, 'attack_rate': 0.0}\n",
    "        \n",
    "        total_urls = len(results)\n",
    "        suspicious_count = sum(1 for r in results if r['is_suspicious'])\n",
    "        \n",
    "        # Count attack types\n",
    "        attack_type_counts = {}\n",
    "        severity_counts = {}\n",
    "        \n",
    "        for result in results:\n",
    "            # Count severity\n",
    "            severity = result['severity']\n",
    "            severity_counts[severity] = severity_counts.get(severity, 0) + 1\n",
    "            \n",
    "            # Count attack types\n",
    "            for attack_type in result['attack_types']:\n",
    "                attack_type_counts[attack_type] = attack_type_counts.get(attack_type, 0) + 1\n",
    "        \n",
    "        # Multi-attack URLs\n",
    "        multi_attack_count = sum(1 for r in results if r['attack_count'] > 1)\n",
    "        \n",
    "        # Average confidence for suspicious URLs\n",
    "        suspicious_results = [r for r in results if r['is_suspicious']]\n",
    "        avg_confidence = (sum(r['confidence'] for r in suspicious_results) / len(suspicious_results)) if suspicious_results else 0.0\n",
    "        \n",
    "        # Attack combinations\n",
    "        attack_combinations = {}\n",
    "        for result in results:\n",
    "            if result['attack_types']:\n",
    "                combo = ', '.join(sorted(result['attack_types']))\n",
    "                attack_combinations[combo] = attack_combinations.get(combo, 0) + 1\n",
    "        \n",
    "        # Sort and get top combinations\n",
    "        top_combinations = dict(sorted(attack_combinations.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "        \n",
    "        return {\n",
    "            'total_urls': total_urls,\n",
    "            'suspicious_urls': suspicious_count,\n",
    "            'clean_urls': total_urls - suspicious_count,\n",
    "            'attack_rate': (suspicious_count / total_urls * 100) if total_urls > 0 else 0.0,\n",
    "            'multi_attack_urls': multi_attack_count,\n",
    "            'severity_distribution': severity_counts,\n",
    "            'attack_distribution': attack_type_counts,\n",
    "            'average_confidence': round(avg_confidence, 3),\n",
    "            'high_confidence_attacks': sum(1 for r in suspicious_results if r['confidence'] >= 0.8),\n",
    "            'top_attack_combinations': top_combinations\n",
    "        }\n",
    "\n",
    "\n",
    "def process_urls_optimized(df: pd.DataFrame, patterns_dict: Dict[str, re.Pattern], n_jobs: int = 4) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Optimized function to process URLs with proper error handling and progress tracking.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting optimized URL security analysis...\")\n",
    "    print(\"Processing a large dataset — analysis time will vary based on system resources.\\n\")\n",
    "\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = VectorizedURLAnalyzer(patterns_dict)\n",
    "    \n",
    "    # Extract URLs\n",
    "    urls = df['request_url'].fillna('/EMPTY_URL').astype(str).tolist()\n",
    "    \n",
    "    # Analyze URLs\n",
    "    try:\n",
    "        analysis_results = analyzer.analyze_batch_parallel(urls, n_jobs=n_jobs)\n",
    "        print(\"Analysis completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "        # Fallback to single-threaded analysis\n",
    "        print(\"Falling back to single-threaded analysis...\")\n",
    "        analysis_results = analyzer.batch_analyze_chunk(urls)\n",
    "    \n",
    "    # Convert results to DataFrame columns\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for i, result in enumerate(analysis_results):\n",
    "        if i < len(result_df):  # Safety check\n",
    "            result_df.at[i, 'is_suspicious'] = result['is_suspicious']\n",
    "            result_df.at[i, 'attack_types'] = ', '.join(result['attack_types'])\n",
    "            result_df.at[i, 'attack_count'] = result['attack_count']\n",
    "            result_df.at[i, 'severity'] = result['severity']\n",
    "            result_df.at[i, 'severity_score'] = result['severity_score']\n",
    "            result_df.at[i, 'confidence'] = result['confidence']\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = analyzer.get_summary_stats(analysis_results)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nQuick Analysis Summary:\")\n",
    "    print(f\"   Total URLs: {summary['total_urls']:,}\")\n",
    "    print(f\"   Suspicious URLs: {summary['suspicious_urls']:,} ({summary['attack_rate']:.2f}%)\")\n",
    "    print(f\"   Multi-attack URLs: {summary['multi_attack_urls']:,}\")\n",
    "    print(f\"   Average confidence: {summary['average_confidence']:.3f}\")\n",
    "    \n",
    "    if summary['attack_distribution']:\n",
    "        print(f\"\\nPreliminary Top Attack Types:\")\n",
    "        for attack_type, count in list(summary['attack_distribution'].items())[:5]:\n",
    "            print(f\"   • {attack_type}: {count:,}\")\n",
    "    \n",
    "    return result_df, summary\n",
    "\n",
    "# Simplified processing function for immediate use\n",
    "def quick_security_analysis(df: pd.DataFrame, patterns_dict: Dict[str, re.Pattern]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Quick security analysis function.\n",
    "    \"\"\"\n",
    "    print(\"Running quick security analysis...\")\n",
    "    \n",
    "    analyzer = VectorizedURLAnalyzer(patterns_dict)\n",
    "    \n",
    "    # Process URLs one by one with progress indication\n",
    "    total_urls = len(df)\n",
    "    results = []\n",
    "    \n",
    "    for i, url in enumerate(df['request_url'].fillna('/EMPTY_URL').astype(str)):\n",
    "        if i % 1000 == 0:  # Progress update every 1000 URLs\n",
    "            print(f\"Progress: {i:,}/{total_urls:,} ({i/total_urls*100:.1f}%)\")\n",
    "        \n",
    "        result = analyzer.analyze_single_url(url)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    result_df = df.copy()\n",
    "    for i, result in enumerate(results):\n",
    "        result_df.at[i, 'is_suspicious'] = result['is_suspicious']\n",
    "        result_df.at[i, 'attack_types'] = ', '.join(result['attack_types'])\n",
    "        result_df.at[i, 'attack_count'] = result['attack_count']\n",
    "        result_df.at[i, 'severity'] = result['severity']\n",
    "        result_df.at[i, 'severity_score'] = result['severity_score']\n",
    "        result_df.at[i, 'confidence'] = result['confidence']\n",
    "    \n",
    "    print(\"Quick analysis completed!\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def optimize_patterns_for_vectorization(patterns: Dict[str, re.Pattern]) -> Dict[str, re.Pattern]:\n",
    "    \"\"\"\n",
    "    Optimize regex patterns for better vectorized performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patterns : Dict[str, re.Pattern]\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, re.Pattern] : Optimized patterns\n",
    "    \"\"\"\n",
    "    optimized = {}\n",
    "    \n",
    "    for name, pattern in patterns.items():\n",
    "        optimized[name] = pattern\n",
    "    \n",
    "    return optimized\n",
    "\n",
    "    \n",
    "class DataProcessor:\n",
    "   \"\"\"\n",
    "   Handles data loading, cleaning, and preprocessing operations.\n",
    "   Designed for reusability across different data formats and sources.\n",
    "   \"\"\"\n",
    "   \n",
    "   @staticmethod\n",
    "   def load_honeypot_data(file_path: str, \n",
    "                         encoding: str = 'utf8') -> pd.DataFrame:\n",
    "       \"\"\"\n",
    "       Load honeypot log data with error handling.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       file_path : str\n",
    "           Path to the CSV file\n",
    "       encoding : str\n",
    "           File encoding (default: utf8)\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       pd.DataFrame : Loaded and initially cleaned dataframe\n",
    "       \"\"\"\n",
    "       try:\n",
    "           # Try different separators if tab doesn't work\n",
    "           separators = ['\\t', ',', '|', ';']\n",
    "           \n",
    "           for sep in separators:\n",
    "               try:\n",
    "                   df = pd.read_csv(\n",
    "                       file_path, \n",
    "                       sep=sep, \n",
    "                       encoding=encoding,\n",
    "                       dtype='string',\n",
    "                       low_memory=False,\n",
    "                       on_bad_lines='skip'\n",
    "                   )\n",
    "                   \n",
    "                   # Check if reasonable data was retrieved\n",
    "                   if len(df.columns) > 5:  # valid data has multiple columns\n",
    "                       print(f\"Successfully loaded data using separator: '{sep}'\")\n",
    "                       break\n",
    "               except:\n",
    "                   continue\n",
    "           else:\n",
    "               # If no separator worked, this tries to detect it\n",
    "               with open(file_path, 'r', encoding=encoding) as f:\n",
    "                   first_line = f.readline()\n",
    "                   # Try to detect separator from first line\n",
    "                   for sep in separators:\n",
    "                       if sep in first_line:\n",
    "                           df = pd.read_csv(\n",
    "                               file_path, \n",
    "                               sep=sep, \n",
    "                               encoding=encoding,\n",
    "                               dtype='string',\n",
    "                               low_memory=False,\n",
    "                               on_bad_lines='skip'\n",
    "                           )\n",
    "                           break\n",
    "           \n",
    "           # Remove any empty rows\n",
    "           df = df.dropna(how='all')\n",
    "           \n",
    "           # Remove trailing empty row if it exists\n",
    "           if df.iloc[-1].isna().all():\n",
    "               df = df[:-1]\n",
    "           \n",
    "           return df\n",
    "           \n",
    "       except Exception as e:\n",
    "           print(f\"Error loading data: {str(e)}\")\n",
    "           raise\n",
    "   \n",
    "   @staticmethod\n",
    "   def clean_data(df: pd.DataFrame, \n",
    "                 required_columns: List[str]) -> pd.DataFrame:\n",
    "       \"\"\"\n",
    "       Clean and prepare dataframe for analysis.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       df : pd.DataFrame\n",
    "           Raw dataframe\n",
    "       required_columns : List[str]\n",
    "           List of required column names\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       pd.DataFrame : Cleaned dataframe\n",
    "       \"\"\"\n",
    "       # Check for required columns\n",
    "       missing_columns = set(required_columns) - set(df.columns)\n",
    "       if missing_columns:\n",
    "           print(f\"Warning: Missing columns: {missing_columns}\")\n",
    "       \n",
    "       # Convert timestamp if present\n",
    "       if 'timestamp' in df.columns:\n",
    "           df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "       \n",
    "       # Clean string columns\n",
    "       for col in df.columns:\n",
    "            if df[col].dtype == 'object' or df[col].dtype.name == 'string':\n",
    "                df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "\n",
    "       # Handle missing URLs\n",
    "       if 'request_url' in df.columns:\n",
    "           df['request_url'] = df['request_url'].fillna('/EMPTY_URL')\n",
    "           df['request_url'] = df['request_url'].replace('', '/EMPTY_URL')\n",
    "       \n",
    "       # Convert numeric columns\n",
    "       numeric_mappings = {\n",
    "           'source_port': 'int32',\n",
    "           'destination_port': 'int32',\n",
    "           'unixTimestamp': 'float64'\n",
    "       }\n",
    "       \n",
    "       for col, dtype in numeric_mappings.items():\n",
    "           if col in df.columns:\n",
    "               df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "       \n",
    "       return df\n",
    "   \n",
    "   @staticmethod\n",
    "   def enrich_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "       \"\"\"\n",
    "       Add derived features to enhance analysis.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       df : pd.DataFrame\n",
    "           Cleaned dataframe\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       pd.DataFrame : Enriched dataframe\n",
    "       \"\"\"\n",
    "       # Extract hour from timestamp for analysis\n",
    "       if 'timestamp' in df.columns:\n",
    "           df['hour'] = df['timestamp'].dt.hour\n",
    "           df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "           df['date'] = df['timestamp'].dt.date\n",
    "       \n",
    "       # Extract URL components\n",
    "       if 'request_url' in df.columns:\n",
    "           # Extract file extension\n",
    "           df['file_extension'] = df['request_url'].str.extract(r'\\.([a-zA-Z0-9]+)(?:\\?|$)')\n",
    "           \n",
    "           # Extract query parameters presence\n",
    "           df['has_query_params'] = df['request_url'].str.contains('\\?', na=False)\n",
    "           \n",
    "           # URL length\n",
    "           df['url_length'] = df['request_url'].str.len()\n",
    "           \n",
    "           # Count special characters\n",
    "           df['special_char_count'] = df['request_url'].str.count('[^a-zA-Z0-9/.\\-_]')\n",
    "       \n",
    "       # IP address analysis\n",
    "       if 'source_ip' in df.columns:\n",
    "           # Extract first octet for class identification\n",
    "           df['ip_first_octet'] = df['source_ip'].str.extract(r'^(\\d+)\\.').astype('Int32')\n",
    "           \n",
    "           # Identify private IPs\n",
    "           df['is_private_ip'] = df['source_ip'].str.match(\n",
    "               r'^(10\\.|172\\.(1[6-9]|2[0-9]|3[01])\\.|192\\.168\\.)'\n",
    "           )\n",
    "       \n",
    "       return df\n",
    "\n",
    "\n",
    "class ReportGenerator:\n",
    "   \"\"\"\n",
    "   Generates professional reports and visualizations from analysis results.\n",
    "   \"\"\"\n",
    "   \n",
    "   def __init__(self, output_dir: str):\n",
    "       \"\"\"\n",
    "       Initialize report generator with output directory.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       output_dir : str\n",
    "           Directory to save reports and visualizations\n",
    "       \"\"\"\n",
    "       self.output_dir = output_dir\n",
    "       self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "       \n",
    "       # Create output directory if it doesn't exist\n",
    "       os.makedirs(output_dir, exist_ok=True)\n",
    "   \n",
    "   def generate_html_report(self, df: pd.DataFrame, \n",
    "                          summary: Dict[str, Any]) -> str:\n",
    "       \"\"\"\n",
    "       Generate a professional HTML report.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       df : pd.DataFrame\n",
    "           Analyzed dataframe\n",
    "       summary : dict\n",
    "           Summary statistics\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       str : Path to generated HTML report\n",
    "       \"\"\"\n",
    "       html_content = f\"\"\"\n",
    "       <!DOCTYPE html>\n",
    "       <html lang=\"en\">\n",
    "       <head>\n",
    "           <meta charset=\"UTF-8\">\n",
    "           <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "           <title>Honeypot Security Analysis Report</title>\n",
    "           <style>\n",
    "               body {{\n",
    "                   font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n",
    "                   line-height: 1.6;\n",
    "                   color: #333;\n",
    "                   max-width: 1200px;\n",
    "                   margin: 0 auto;\n",
    "                   padding: 20px;\n",
    "                   background-color: #f5f5f5;\n",
    "               }}\n",
    "               .header {{\n",
    "                   background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                   color: white;\n",
    "                   padding: 30px;\n",
    "                   border-radius: 10px;\n",
    "                   margin-bottom: 30px;\n",
    "                   box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "               }}\n",
    "               .header h1 {{\n",
    "                   margin: 0;\n",
    "                   font-size: 2.5em;\n",
    "               }}\n",
    "               .header p {{\n",
    "                   margin: 10px 0 0 0;\n",
    "                   opacity: 0.9;\n",
    "               }}\n",
    "               .metric-grid {{\n",
    "                   display: grid;\n",
    "                   grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "                   gap: 20px;\n",
    "                   margin-bottom: 30px;\n",
    "               }}\n",
    "               .metric-card {{\n",
    "                   background: white;\n",
    "                   padding: 20px;\n",
    "                   border-radius: 8px;\n",
    "                   box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "                   text-align: center;\n",
    "                   transition: transform 0.2s;\n",
    "               }}\n",
    "               .metric-card:hover {{\n",
    "                   transform: translateY(-2px);\n",
    "                   box-shadow: 0 4px 8px rgba(0,0,0,0.15);\n",
    "               }}\n",
    "               .metric-value {{\n",
    "                   font-size: 2.5em;\n",
    "                   font-weight: bold;\n",
    "                   color: #667eea;\n",
    "                   margin: 10px 0;\n",
    "               }}\n",
    "               .metric-label {{\n",
    "                   color: #666;\n",
    "                   font-size: 0.9em;\n",
    "                   text-transform: uppercase;\n",
    "                   letter-spacing: 1px;\n",
    "               }}\n",
    "               .section {{\n",
    "                   background: white;\n",
    "                   padding: 30px;\n",
    "                   border-radius: 8px;\n",
    "                   margin-bottom: 20px;\n",
    "                   box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "               }}\n",
    "               .section h2 {{\n",
    "                   color: #667eea;\n",
    "                   border-bottom: 2px solid #e2e8f0;\n",
    "                   padding-bottom: 10px;\n",
    "                   margin-bottom: 20px;\n",
    "               }}\n",
    "               .severity-badge {{\n",
    "                   display: inline-block;\n",
    "                   padding: 4px 12px;\n",
    "                   border-radius: 20px;\n",
    "                   font-size: 0.85em;\n",
    "                   font-weight: bold;\n",
    "                   text-transform: uppercase;\n",
    "                   margin: 2px;\n",
    "               }}\n",
    "               .severity-critical {{\n",
    "                   background-color: #dc2626;\n",
    "                   color: white;\n",
    "               }}\n",
    "               .severity-high {{\n",
    "                   background-color: #f97316;\n",
    "                   color: white;\n",
    "               }}\n",
    "               .severity-medium {{\n",
    "                   background-color: #eab308;\n",
    "                   color: white;\n",
    "               }}\n",
    "               .severity-low {{\n",
    "                   background-color: #3b82f6;\n",
    "                   color: white;\n",
    "               }}\n",
    "               .severity-clean {{\n",
    "                   background-color: #10b981;\n",
    "                   color: white;\n",
    "               }}\n",
    "               table {{\n",
    "                   width: 100%;\n",
    "                   border-collapse: collapse;\n",
    "                   margin-top: 20px;\n",
    "               }}\n",
    "               th {{\n",
    "                   background-color: #f3f4f6;\n",
    "                   color: #374151;\n",
    "                   font-weight: 600;\n",
    "                   text-align: left;\n",
    "                   padding: 12px;\n",
    "                   border-bottom: 2px solid #e5e7eb;\n",
    "               }}\n",
    "               td {{\n",
    "                   padding: 12px;\n",
    "                   border-bottom: 1px solid #e5e7eb;\n",
    "               }}\n",
    "               tr:hover {{\n",
    "                   background-color: #f9fafb;\n",
    "               }}\n",
    "               .chart-container {{\n",
    "                   margin: 20px 0;\n",
    "                   text-align: center;\n",
    "               }}\n",
    "               .footer {{\n",
    "                   text-align: center;\n",
    "                   color: #666;\n",
    "                   margin-top: 50px;\n",
    "                   padding-top: 20px;\n",
    "                   border-top: 1px solid #e2e8f0;\n",
    "               }}\n",
    "               .alert {{\n",
    "                   padding: 15px;\n",
    "                   border-radius: 5px;\n",
    "                   margin: 20px 0;\n",
    "               }}\n",
    "               .alert-warning {{\n",
    "                   background-color: #fef3c7;\n",
    "                   border: 1px solid #fbbf24;\n",
    "                   color: #92400e;\n",
    "               }}\n",
    "               .alert-danger {{\n",
    "                   background-color: #fee2e2;\n",
    "                   border: 1px solid #f87171;\n",
    "                   color: #991b1b;\n",
    "               }}\n",
    "               .progress-bar {{\n",
    "                   width: 100%;\n",
    "                   height: 20px;\n",
    "                   background-color: #e5e7eb;\n",
    "                   border-radius: 10px;\n",
    "                   overflow: hidden;\n",
    "                   margin: 10px 0;\n",
    "               }}\n",
    "               .progress-fill {{\n",
    "                   height: 100%;\n",
    "                   background: linear-gradient(90deg, #10b981 0%, #3b82f6 100%);\n",
    "                   transition: width 0.3s ease;\n",
    "               }}\n",
    "           </style>\n",
    "       </head>\n",
    "       <body>\n",
    "           <div class=\"header\">\n",
    "               <h1>Honeypot Security Analysis Report</h1>\n",
    "               <p>Generated on {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}</p>\n",
    "               <p>Analysis by: Kyle Purves</p>\n",
    "           </div>\n",
    "           \n",
    "           <div class=\"metric-grid\">\n",
    "               <div class=\"metric-card\">\n",
    "                   <div class=\"metric-label\">Total Requests</div>\n",
    "                   <div class=\"metric-value\">{summary['total_urls']:,}</div>\n",
    "               </div>\n",
    "               <div class=\"metric-card\">\n",
    "                   <div class=\"metric-label\">Suspicious Requests</div>\n",
    "                   <div class=\"metric-value\">{summary['suspicious_urls']:,}</div>\n",
    "               </div>\n",
    "               <div class=\"metric-card\">\n",
    "                   <div class=\"metric-label\">Attack Rate</div>\n",
    "                   <div class=\"metric-value\">\n",
    "                       {(summary['suspicious_urls'] / summary['total_urls'] * 100):.1f}%\n",
    "                   </div>\n",
    "               </div>\n",
    "               <div class=\"metric-card\">\n",
    "                   <div class=\"metric-label\">Multi-Attack URLs</div>\n",
    "                   <div class=\"metric-value\">{summary['multi_attack_urls']:,}</div>\n",
    "               </div>\n",
    "           </div>\n",
    "       \"\"\"\n",
    "       \n",
    "       # Add severity distribution\n",
    "       if summary['suspicious_urls'] > 0:\n",
    "           html_content += \"\"\"\n",
    "           <div class=\"section\">\n",
    "               <h2>Severity Distribution</h2>\n",
    "               <div class=\"progress-bar\">\n",
    "           \"\"\"\n",
    "           \n",
    "           severity_colors = {\n",
    "               'Critical': '#dc2626',\n",
    "               'High': '#f97316',\n",
    "               'Medium': '#eab308',\n",
    "               'Low': '#3b82f6'\n",
    "           }\n",
    "           \n",
    "\n",
    "           html_content += '<div style=\"display: flex; width: 100%; height: 20px; border-radius: 10px; overflow: hidden;\">'\n",
    "           for severity, count in summary['severity_distribution'].items():\n",
    "               if severity != 'Clean' and count > 0:\n",
    "                   width = (count / summary['total_urls']) * 100\n",
    "                   html_content += f\"\"\"\n",
    "                   <div style=\"\n",
    "                       width: {width}%; \n",
    "                       height: 100%;\n",
    "                       background: {severity_colors.get(severity, '#666')};\n",
    "                    \"></div>\n",
    "                    \"\"\"\n",
    "           \n",
    "           html_content += \"\"\"\n",
    "               </div>\n",
    "               <table>\n",
    "                   <thead>\n",
    "                       <tr>\n",
    "                           <th>Severity Level</th>\n",
    "                           <th>Count</th>\n",
    "                           <th>Percentage</th>\n",
    "                       </tr>\n",
    "                   </thead>\n",
    "                   <tbody>\n",
    "           \"\"\"\n",
    "           \n",
    "           for severity, count in summary['severity_distribution'].items():\n",
    "               if count > 0:\n",
    "                   percentage = (count / summary['total_urls']) * 100\n",
    "                   badge_class = f\"severity-{severity.lower()}\"\n",
    "                   html_content += f\"\"\"\n",
    "                   <tr>\n",
    "                       <td><span class=\"severity-badge {badge_class}\">{severity}</span></td>\n",
    "                       <td>{count:,}</td>\n",
    "                       <td>{percentage:.2f}%</td>\n",
    "                   </tr>\n",
    "                   \"\"\"\n",
    "           \n",
    "           html_content += \"\"\"\n",
    "                   </tbody>\n",
    "               </table>\n",
    "           </div>\n",
    "           \"\"\"\n",
    "       \n",
    "       # Add attack type distribution\n",
    "       if summary['attack_distribution']:\n",
    "           html_content += \"\"\"\n",
    "           <div class=\"section\">\n",
    "               <h2>Attack Type Distribution</h2>\n",
    "               <table>\n",
    "                   <thead>\n",
    "                       <tr>\n",
    "                           <th>Attack Type</th>\n",
    "                           <th>Occurrences</th>\n",
    "                           <th>Percentage of Attacks</th>\n",
    "                       </tr>\n",
    "                   </thead>\n",
    "                   <tbody>\n",
    "           \"\"\"\n",
    "           \n",
    "           total_attacks = sum(summary['attack_distribution'].values())\n",
    "           for attack_type, count in sorted(\n",
    "               summary['attack_distribution'].items(), \n",
    "               key=lambda x: x[1], \n",
    "               reverse=True\n",
    "           ):\n",
    "               percentage = (count / total_attacks) * 100\n",
    "               html_content += f\"\"\"\n",
    "               <tr>\n",
    "                   <td>{attack_type}</td>\n",
    "                   <td>{count:,}</td>\n",
    "                   <td>{percentage:.2f}%</td>\n",
    "               </tr>\n",
    "               \"\"\"\n",
    "           \n",
    "           html_content += \"\"\"\n",
    "                   </tbody>\n",
    "               </table>\n",
    "           </div>\n",
    "           \"\"\"\n",
    "       \n",
    "       # Add top attack combinations\n",
    "       if summary['top_attack_combinations']:\n",
    "           html_content += \"\"\"\n",
    "           <div class=\"section\">\n",
    "               <h2>Top Attack Combinations</h2>\n",
    "               <p>URLs often contain multiple attack vectors. Here are the most common combinations:</p>\n",
    "               <table>\n",
    "                   <thead>\n",
    "                       <tr>\n",
    "                           <th>Attack Combination</th>\n",
    "                           <th>Frequency</th>\n",
    "                       </tr>\n",
    "                   </thead>\n",
    "                   <tbody>\n",
    "           \"\"\"\n",
    "           \n",
    "           for combo, count in list(summary['top_attack_combinations'].items())[:10]:\n",
    "               html_content += f\"\"\"\n",
    "               <tr>\n",
    "                   <td>{combo}</td>\n",
    "                   <td>{count:,}</td>\n",
    "               </tr>\n",
    "               \"\"\"\n",
    "           \n",
    "           html_content += \"\"\"\n",
    "                   </tbody>\n",
    "               </table>\n",
    "           </div>\n",
    "           \"\"\"\n",
    "       \n",
    "       # Add critical findings\n",
    "       critical_count = summary['severity_distribution'].get('Critical', 0)\n",
    "       if critical_count > 0:\n",
    "           html_content += f\"\"\"\n",
    "           <div class=\"alert alert-danger\">\n",
    "               <h3>Critical Security Alert</h3>\n",
    "               <p><strong>{critical_count}</strong> critical severity attacks detected requiring immediate attention!</p>\n",
    "               <p>These attacks pose the highest risk to system security and should be investigated immediately.</p>\n",
    "           </div>\n",
    "           \"\"\"\n",
    "       \n",
    "       # Add insights and recommendations\n",
    "       html_content += \"\"\"\n",
    "       <div class=\"section\">\n",
    "           <h2>Key Insights</h2>\n",
    "           <ul>\n",
    "       \"\"\"\n",
    "       \n",
    "       # Generate insights based on data\n",
    "       insights = []\n",
    "       \n",
    "       if summary['average_confidence'] > 0.8:\n",
    "           insights.append(\"High confidence in attack detection (>80%) indicates clear attack patterns\")\n",
    "       \n",
    "       if summary['multi_attack_urls'] > summary['suspicious_urls'] * 0.3:\n",
    "           insights.append(\"Significant number of multi-vector attacks suggest sophisticated attackers\")\n",
    "       \n",
    "       attack_variety = len(summary['attack_distribution'])\n",
    "       if attack_variety >= 7:\n",
    "           insights.append(f\"Wide variety of attack types detected ({attack_variety} different types)\")\n",
    "       \n",
    "       if 'SQL Injection' in summary['attack_distribution'] and \\\n",
    "          summary['attack_distribution']['SQL Injection'] > total_attacks * 0.4:\n",
    "           insights.append(\"SQL Injection is the predominant attack vector. Database security should be prioritized.\")\n",
    "       \n",
    "       for insight in insights:\n",
    "           html_content += f\"<li>{insight}</li>\"\n",
    "       \n",
    "       html_content += \"\"\"\n",
    "           </ul>\n",
    "       </div>\n",
    "       \n",
    "       <div class=\"section\">\n",
    "           <h2>Security Recommendations</h2>\n",
    "           <ol>\n",
    "       \"\"\"\n",
    "       \n",
    "       # Generate recommendations based on findings\n",
    "       recommendations = [\n",
    "           \"Implement a Web Application Firewall (WAF) to filter malicious requests\",\n",
    "           \"Enable comprehensive logging and real-time monitoring\",\n",
    "           \"Regularly update all software components with security patches\",\n",
    "           \"Conduct security awareness training for development teams\",\n",
    "           \"Implement rate limiting to prevent automated attacks\",\n",
    "           \"Use parameterized queries to prevent SQL injection\",\n",
    "           \"Deploy Content Security Policy (CSP) headers for XSS protection\",\n",
    "           \"Perform regular security audits and penetration testing\"\n",
    "       ]\n",
    "       \n",
    "       for rec in recommendations[:5]:\n",
    "           html_content += f\"<li>{rec}</li>\"\n",
    "       \n",
    "       html_content += \"\"\"\n",
    "           </ol>\n",
    "       </div>\n",
    "       \n",
    "       <div class=\"footer\">\n",
    "           <p>Report generated by URL Security Analyzer</p>\n",
    "           <p>&copy; 2025 Kyle Purves.</p>\n",
    "       </div>\n",
    "       \n",
    "       </body>\n",
    "       </html>\n",
    "       \"\"\"\n",
    "       \n",
    "       # Save HTML report\n",
    "       report_path = os.path.join(self.output_dir, f'security_report_{self.timestamp}.html')\n",
    "       with open(report_path, 'w', encoding='utf-8') as f:\n",
    "           f.write(html_content)\n",
    "       \n",
    "       return report_path\n",
    "   \n",
    "   def create_visualizations(self, df: pd.DataFrame, summary: Dict[str, Any]) -> List[str]:\n",
    "       \"\"\"\n",
    "       Create visualizations for the report.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       df : pd.DataFrame\n",
    "           Analyzed dataframe\n",
    "       summary : dict\n",
    "           Summary statistics\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       List[str] : Paths to generated visualization files\n",
    "       \"\"\"\n",
    "       viz_paths = []\n",
    "       \n",
    "       # Set style\n",
    "       plt.style.use('seaborn-v0_8-darkgrid')\n",
    "       colors = ['#667eea', '#764ba2', '#f093fb', '#4facfe', '#00f2fe']\n",
    "       \n",
    "       # 1. Attack Type Distribution - Horizontal Bar Chart\n",
    "       if summary['attack_distribution']:\n",
    "           plt.figure(figsize=(10, 6))\n",
    "           attack_types = list(summary['attack_distribution'].keys())\n",
    "           counts = list(summary['attack_distribution'].values())\n",
    "           \n",
    "           # Sort by count\n",
    "           sorted_data = sorted(zip(attack_types, counts), key=lambda x: x[1])\n",
    "           attack_types, counts = zip(*sorted_data)\n",
    "           \n",
    "           bars = plt.barh(attack_types, counts, color=colors[0])\n",
    "           \n",
    "           # Add value labels on bars\n",
    "           for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "               plt.text(bar.get_width() + max(counts)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                       f'{count:,}', va='center', fontweight='bold')\n",
    "           \n",
    "           plt.xlabel('Number of Occurrences', fontsize=12, fontweight='bold')\n",
    "           plt.title('Distribution of Attack Types Detected', fontsize=16, fontweight='bold', pad=20)\n",
    "           plt.tight_layout()\n",
    "           \n",
    "           path = os.path.join(self.output_dir, f'attack_distribution_{self.timestamp}.png')\n",
    "           plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "           plt.close()\n",
    "           viz_paths.append(path)\n",
    "       \n",
    "       # 2. Severity Distribution - Donut Chart\n",
    "       plt.figure(figsize=(8, 8))\n",
    "       severity_data = [(k, v) for k, v in summary['severity_distribution'].items() if v > 0]\n",
    "       if severity_data:\n",
    "           labels, sizes = zip(*severity_data)\n",
    "           \n",
    "           # Colors for severity levels\n",
    "           severity_colors = {\n",
    "               'Critical': '#dc2626',\n",
    "               'High': '#f97316',\n",
    "               'Medium': '#eab308',\n",
    "               'Low': '#3b82f6',\n",
    "               'Clean': '#10b981'\n",
    "           }\n",
    "           colors_list = [severity_colors.get(label, '#666') for label in labels]\n",
    "           \n",
    "           # Create donut chart\n",
    "           wedges, texts, autotexts = plt.pie(sizes, labels=labels, colors=colors_list, \n",
    "                                              autopct='%1.1f%%', startangle=90,\n",
    "                                              wedgeprops=dict(width=0.5, edgecolor='white'))\n",
    "           \n",
    "           # Enhance text\n",
    "           for text in texts:\n",
    "               text.set_fontsize(12)\n",
    "               text.set_fontweight('bold')\n",
    "           for autotext in autotexts:\n",
    "               autotext.set_color('white')\n",
    "               autotext.set_fontsize(10)\n",
    "               autotext.set_fontweight('bold')\n",
    "           \n",
    "           plt.title('Request Severity Distribution', fontsize=16, fontweight='bold', pad=20)\n",
    "           \n",
    "           # Add total in center\n",
    "           plt.text(0, 0, f'Total\\n{sum(sizes):,}', ha='center', va='center', \n",
    "                   fontsize=20, fontweight='bold')\n",
    "           \n",
    "           path = os.path.join(self.output_dir, f'severity_distribution_{self.timestamp}.png')\n",
    "           plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "           plt.close()\n",
    "           viz_paths.append(path)\n",
    "       \n",
    "       # 3. Temporal Analysis - If timestamp data available\n",
    "       if 'hour' in df.columns and df['is_suspicious'].sum() > 0:\n",
    "           plt.figure(figsize=(12, 6))\n",
    "           \n",
    "           # Group by hour\n",
    "           hourly_attacks = df[df['is_suspicious']].groupby('hour').size()\n",
    "           hours = list(range(24))\n",
    "           counts = [hourly_attacks.get(h, 0) for h in hours]\n",
    "           \n",
    "           # Create bar chart with gradient colors\n",
    "           bars = plt.bar(hours, counts)\n",
    "           \n",
    "           # Apply gradient colors\n",
    "           for i, bar in enumerate(bars):\n",
    "               bar.set_color(plt.cm.plasma(i / 23))\n",
    "           \n",
    "           plt.xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
    "           plt.ylabel('Number of Attacks', fontsize=12, fontweight='bold')\n",
    "           plt.title('Attack Distribution by Hour of Day', fontsize=16, fontweight='bold', pad=20)\n",
    "           plt.xticks(hours)\n",
    "           plt.grid(axis='y', alpha=0.3)\n",
    "           \n",
    "           # Add average line\n",
    "           avg_attacks = sum(counts) / 24\n",
    "           plt.axhline(y=avg_attacks, color='red', linestyle='--', \n",
    "                      label=f'Average: {avg_attacks:.1f}')\n",
    "           plt.legend()\n",
    "           \n",
    "           path = os.path.join(self.output_dir, f'temporal_analysis_{self.timestamp}.png')\n",
    "           plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "           plt.close()\n",
    "           viz_paths.append(path)\n",
    "       \n",
    "       # 4. Top Attacking Countries - If country data available\n",
    "       if 'country' in df.columns and df['is_suspicious'].sum() > 0:\n",
    "           plt.figure(figsize=(10, 6))\n",
    "           \n",
    "           # Get top 10 attacking countries\n",
    "           country_counts = df[df['is_suspicious']]['country'].value_counts().head(10)\n",
    "           \n",
    "           # Create horizontal bar chart\n",
    "           plt.barh(country_counts.index[::-1], country_counts.values[::-1], \n",
    "                   color=plt.cm.viridis(np.linspace(0.2, 0.8, 10)))\n",
    "           \n",
    "           plt.xlabel('Number of Attacks', fontsize=12, fontweight='bold')\n",
    "           plt.title('Top 10 Attacking Countries', fontsize=16, fontweight='bold', pad=20)\n",
    "           \n",
    "           # Add value labels\n",
    "           for i, v in enumerate(country_counts.values[::-1]):\n",
    "               plt.text(v + max(country_counts)*0.01, i, f'{v:,}', \n",
    "                       va='center', fontweight='bold')\n",
    "           \n",
    "           plt.tight_layout()\n",
    "           \n",
    "           path = os.path.join(self.output_dir, f'country_distribution_{self.timestamp}.png')\n",
    "           plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "           plt.close()\n",
    "           viz_paths.append(path)\n",
    "       \n",
    "       return viz_paths\n",
    "   \n",
    "   def export_results(self, df: pd.DataFrame, summary: Dict[str, Any]) -> Dict[str, str]:\n",
    "       \"\"\"\n",
    "       Export analysis results in multiple formats.\n",
    "       \n",
    "       Parameters:\n",
    "       -----------\n",
    "       df : pd.DataFrame\n",
    "           Analyzed dataframe\n",
    "       summary : dict\n",
    "           Summary statistics\n",
    "           \n",
    "       Returns:\n",
    "       --------\n",
    "       dict : Paths to exported files\n",
    "       \"\"\"\n",
    "       export_paths = {}\n",
    "       \n",
    "       # 1. Export full results CSV\n",
    "       full_path = os.path.join(self.output_dir, f'full_analysis_{self.timestamp}.csv')\n",
    "       df.to_csv(full_path, index=False)\n",
    "       export_paths['full_csv'] = full_path\n",
    "       \n",
    "       # 2. Export suspicious-only CSV\n",
    "       if 'is_suspicious' in df.columns:\n",
    "           suspicious_df = df[df['is_suspicious']].copy()\n",
    "           if len(suspicious_df) > 0:\n",
    "               suspicious_path = os.path.join(\n",
    "                   self.output_dir, \n",
    "                   f'suspicious_only_{self.timestamp}.csv'\n",
    "               )\n",
    "               suspicious_df.to_csv(suspicious_path, index=False)\n",
    "               export_paths['suspicious_csv'] = suspicious_path\n",
    "       \n",
    "       # 3. Export critical attacks\n",
    "       if 'severity' in df.columns:\n",
    "           critical_df = df[df['severity'] == 'Critical'].copy()\n",
    "           if len(critical_df) > 0:\n",
    "               critical_path = os.path.join(\n",
    "                   self.output_dir, \n",
    "                   f'critical_attacks_{self.timestamp}.csv'\n",
    "               )\n",
    "               critical_df.to_csv(critical_path, index=False)\n",
    "               export_paths['critical_csv'] = critical_path\n",
    "       \n",
    "       # 4. Export summary JSON\n",
    "       summary_path = os.path.join(self.output_dir, f'summary_{self.timestamp}.json')\n",
    "       with open(summary_path, 'w') as f:\n",
    "           json.dump(summary, f, indent=4, default=str)\n",
    "       export_paths['summary_json'] = summary_path\n",
    "       \n",
    "       # 5. Generate and export executive summary text\n",
    "       exec_summary_path = os.path.join(\n",
    "           self.output_dir, \n",
    "           f'executive_summary_{self.timestamp}.txt'\n",
    "       )\n",
    "       \n",
    "       with open(exec_summary_path, 'w') as f:\n",
    "           f.write(\"EXECUTIVE SUMMARY - HONEYPOT SECURITY ANALYSIS\\n\")\n",
    "           f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "           f.write(f\"Report Date: {datetime.now().strftime('%B %d, %Y')}\\n\")\n",
    "           f.write(f\"Analysis Period: {df['timestamp'].min()} to {df['timestamp'].max()}\\n\")\n",
    "           f.write(f\"Analyst: Kyle Purves\\n\\n\")\n",
    "           \n",
    "           f.write(\"KEY FINDINGS:\\n\")\n",
    "           f.write(\"-\" * 40 + \"\\n\")\n",
    "           f.write(f\"• Total Requests Analyzed: {summary['total_urls']:,}\\n\")\n",
    "           f.write(f\"• Malicious Requests Identified: {summary['suspicious_urls']:,}\\n\")\n",
    "           f.write(f\"• Attack Rate: {(summary['suspicious_urls'] / summary['total_urls'] * 100):.2f}%\\n\")\n",
    "           f.write(f\"• Critical Severity Attacks: {summary['severity_distribution'].get('Critical', 0):,}\\n\")\n",
    "           f.write(f\"• Multi-Vector Attacks: {summary['multi_attack_urls']:,}\\n\\n\")\n",
    "           \n",
    "           f.write(\"TOP THREATS:\\n\")\n",
    "           f.write(\"-\" * 40 + \"\\n\")\n",
    "           for attack_type, count in list(summary['attack_distribution'].items())[:5]:\n",
    "               f.write(f\"• {attack_type}: {count:,} instances\\n\")\n",
    "           \n",
    "           f.write(\"\\nRECOMMENDATIONS:\\n\")\n",
    "           f.write(\"-\" * 40 + \"\\n\")\n",
    "           f.write(\"1. Immediate deployment of Web Application Firewall (WAF)\\n\")\n",
    "           f.write(\"2. Implementation of rate limiting and IP blocking\\n\")\n",
    "           f.write(\"3. Security audit of all web applications\\n\")\n",
    "           f.write(\"4. Enhanced monitoring and alerting systems\\n\")\n",
    "           f.write(\"5. Regular security training for development teams\\n\")\n",
    "       \n",
    "       export_paths['executive_summary'] = exec_summary_path\n",
    "       \n",
    "       return export_paths\n",
    "\n",
    "\n",
    "# Utility functions for extra analysis\n",
    "def calculate_attack_metrics(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "   \"\"\"\n",
    "   Calculate advanced metrics for attack analysis.\n",
    "   \n",
    "   Parameters:\n",
    "   -----------\n",
    "   df : pd.DataFrame\n",
    "       Dataframe with attack analysis results\n",
    "       \n",
    "   Returns:\n",
    "   --------\n",
    "   dict : Advanced metrics\n",
    "   \"\"\"\n",
    "   metrics = {\n",
    "       'total_records': len(df),\n",
    "       'time_range': None,\n",
    "       'attack_velocity': 0,\n",
    "       'peak_attack_time': None,\n",
    "       'most_targeted_paths': [],\n",
    "       'attack_sophistication_index': 0,\n",
    "       'geographic_diversity': 0,\n",
    "       'unique_attackers': 0,\n",
    "       'repeat_offenders': [],\n",
    "       'attack_patterns': {}\n",
    "   }\n",
    "   \n",
    "   # Time-based analysis\n",
    "   if 'timestamp' in df.columns:\n",
    "       df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "       time_range = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600\n",
    "       metrics['time_range'] = f\"{time_range:.1f} hours\"\n",
    "       \n",
    "       if 'is_suspicious' in df.columns:\n",
    "           attacks_per_hour = df[df['is_suspicious']].groupby(\n",
    "               df['timestamp'].dt.floor('H')\n",
    "           ).size()\n",
    "           \n",
    "           if len(attacks_per_hour) > 0:\n",
    "               metrics['attack_velocity'] = attacks_per_hour.mean()\n",
    "               metrics['peak_attack_time'] = attacks_per_hour.idxmax()\n",
    "   \n",
    "   # Path analysis\n",
    "   if 'request_url' in df.columns and 'is_suspicious' in df.columns:\n",
    "       suspicious_df = df[df['is_suspicious']]\n",
    "       if len(suspicious_df) > 0:\n",
    "           # Extract base paths\n",
    "           suspicious_df['base_path'] = suspicious_df['request_url'].str.split('?').str[0]\n",
    "           path_counts = suspicious_df['base_path'].value_counts().head(10)\n",
    "           metrics['most_targeted_paths'] = [\n",
    "               {'path': path, 'count': count} \n",
    "               for path, count in path_counts.items()\n",
    "           ]\n",
    "   \n",
    "   # Sophistication analysis\n",
    "   if 'attack_count' in df.columns:\n",
    "       multi_attack_ratio = len(df[df['attack_count'] > 1]) / len(df[df['is_suspicious']])\n",
    "       avg_attacks_per_url = df[df['is_suspicious']]['attack_count'].mean()\n",
    "       metrics['attack_sophistication_index'] = round(\n",
    "           (multi_attack_ratio * 0.5 + (avg_attacks_per_url - 1) * 0.5) * 10, 2\n",
    "       )\n",
    "   \n",
    "   # Geographic analysis\n",
    "   if 'country' in df.columns and 'is_suspicious' in df.columns:\n",
    "       unique_countries = df[df['is_suspicious']]['country'].nunique()\n",
    "       metrics['geographic_diversity'] = unique_countries\n",
    "   \n",
    "   # Attacker analysis\n",
    "   if 'source_ip' in df.columns and 'is_suspicious' in df.columns:\n",
    "       suspicious_ips = df[df['is_suspicious']]['source_ip']\n",
    "       metrics['unique_attackers'] = suspicious_ips.nunique()\n",
    "       \n",
    "       # Find repeat offenders\n",
    "       ip_counts = suspicious_ips.value_counts()\n",
    "       repeat_offenders = ip_counts[ip_counts > 10].head(10)\n",
    "       metrics['repeat_offenders'] = [\n",
    "           {'ip': ip, 'attacks': count} \n",
    "           for ip, count in repeat_offenders.items()\n",
    "       ]\n",
    "   \n",
    "   # Attack pattern analysis\n",
    "   if 'attack_types' in df.columns:\n",
    "       # Find common attack sequences\n",
    "       attack_sequences = df[df['is_suspicious']]['attack_types'].value_counts().head(10)\n",
    "       metrics['attack_patterns'] = {\n",
    "           seq: count for seq, count in attack_sequences.items() if seq\n",
    "       }\n",
    "   \n",
    "   return metrics\n",
    "\n",
    "\n",
    "def progress_reporter(current: int, total: int) -> None:\n",
    "   \"\"\"\n",
    "   Report progress during batch processing.\n",
    "   \n",
    "   Parameters:\n",
    "   -----------\n",
    "   current : int\n",
    "       Current item number\n",
    "   total : int\n",
    "       Total items to process\n",
    "   \"\"\"\n",
    "   percentage = (current / total) * 100\n",
    "   bar_length = 50\n",
    "   filled_length = int(bar_length * current / total)\n",
    "   bar = '[]' * filled_length + '-' * (bar_length - filled_length)\n",
    "   \n",
    "   print(f'\\rProgress: |{bar}| {percentage:.1f}% ({current:,}/{total:,})', end='')\n",
    "   \n",
    "   if current == total:\n",
    "       print()  \n",
    "\n",
    "\n",
    "# Main execution function\n",
    "def main(data_dir: str, output_dir: str):\n",
    "   \"\"\"\n",
    "   Main execution function that organizes the entire analysis process.\n",
    "   \n",
    "   Parameters:\n",
    "   -----------\n",
    "   data_dir : str\n",
    "       Directory containing the input data\n",
    "   output_dir : str\n",
    "       Directory for output files\n",
    "   \"\"\"\n",
    "   print(\"\\n\" + \"=\"*80)\n",
    "   print(\"HONEYPOT URL SECURITY ANALYSIS SYSTEM\")\n",
    "   print(\"=\"*80)\n",
    "   print(f\"Version: 1.0\")\n",
    "   print(f\"Author: Kyle Purves\")\n",
    "   print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "   print(\"=\"*80 + \"\\n\")\n",
    "   \n",
    "   try:\n",
    "       # Step 1: Load data\n",
    "       print(\"STEP 1: Loading honeypot data...\")\n",
    "       file_path = os.path.join(data_dir, 'log_file_2.csv')\n",
    "       \n",
    "       processor = DataProcessor()\n",
    "       df = processor.load_honeypot_data(file_path)\n",
    "       print(f\"✓ Loaded {len(df):,} records from {df.shape[1]} columns\")\n",
    "       \n",
    "       # Step 2: Clean data\n",
    "       print(\"\\nSTEP 2: Cleaning and preprocessing data...\")\n",
    "       required_columns = ['source_ip', 'request_url', 'timestamp']\n",
    "       df = processor.clean_data(df, required_columns)\n",
    "       df = processor.enrich_data(df)\n",
    "       print(\"Data cleaning complete\")\n",
    "       \n",
    "       # Step 3: Initialize analyzer\n",
    "       print(\"\\nSTEP 3: Initializing security analyzer...\")\n",
    "       analyzer = URLSecurityAnalyzer()\n",
    "       print(\"Analyzer ready with 10 attack detection patterns\")\n",
    "       \n",
    "       # Step 4: Analyze URLs\n",
    "       print(\"\\nSTEP 4: Analyzing URLs for security threats...\")\n",
    "       print(\"This may take a few minutes for large datasets...\")\n",
    "       \n",
    "       # Batch analyze with progress reporting\n",
    "       analysis_results = analyzer.batch_analyze(\n",
    "           df['request_url'].tolist(),\n",
    "           progress_callback=progress_reporter\n",
    "       )\n",
    "       \n",
    "       # Add results to dataframe\n",
    "       for i, result in enumerate(analysis_results):\n",
    "           df.at[i, 'is_suspicious'] = result['is_suspicious']\n",
    "           df.at[i, 'attack_types'] = ', '.join(result['attack_types'])\n",
    "           df.at[i, 'attack_count'] = result['attack_count']\n",
    "           df.at[i, 'severity'] = result['severity']\n",
    "           df.at[i, 'severity_score'] = result['severity_score']\n",
    "           df.at[i, 'confidence'] = result['confidence']\n",
    "       \n",
    "       print(\"\\n✓ URL analysis complete\")\n",
    "       \n",
    "       # Step 5: Generate summary\n",
    "       print(\"\\nSTEP 5: Generating analysis summary...\")\n",
    "       summary = analyzer.get_attack_summary(analysis_results)\n",
    "       \n",
    "       # Calculate additional metrics\n",
    "       advanced_metrics = calculate_attack_metrics(df)\n",
    "       summary.update(advanced_metrics)\n",
    "       \n",
    "       # Print summary\n",
    "       print(\"\\n\" + \"=\"*60)\n",
    "       print(\"ANALYSIS SUMMARY\")\n",
    "       print(\"=\"*60)\n",
    "       print(f\"Total URLs Analyzed: {summary['total_urls']:,}\")\n",
    "       print(f\"Suspicious URLs: {summary['suspicious_urls']:,} ({summary['suspicious_urls']/summary['total_urls']*100:.2f}%)\")\n",
    "       print(f\"Clean URLs: {summary['clean_urls']:,}\")\n",
    "       print(f\"Invalid URLs: {summary['invalid_urls']:,}\")\n",
    "       print(f\"\\nAttack Sophistication Index: {summary.get('attack_sophistication_index', 0)}/10\")\n",
    "       print(f\"Geographic Diversity: {summary.get('geographic_diversity', 0)} countries\")\n",
    "       print(f\"Unique Attackers: {summary.get('unique_attackers', 0):,}\")\n",
    "       \n",
    "       # Step 6: Generate reports\n",
    "       print(\"\\nSTEP 6: Generating reports and visualizations...\")\n",
    "       generator = ReportGenerator(output_dir)\n",
    "       \n",
    "       # Generate HTML report\n",
    "       html_path = generator.generate_html_report(df, summary)\n",
    "       print(f\"HTML report generated: {html_path}\")\n",
    "       \n",
    "       # Create visualizations\n",
    "       viz_paths = generator.create_visualizations(df, summary)\n",
    "       print(f\"Created {len(viz_paths)} visualizations\")\n",
    "       \n",
    "       # Export results\n",
    "       export_paths = generator.export_results(df, summary)\n",
    "       print(f\"Exported {len(export_paths)} data files\")\n",
    "       \n",
    "       # Step 7: Display critical findings\n",
    "       print(\"\\nCRITICAL FINDINGS:\")\n",
    "       print(\"-\" * 40)\n",
    "       \n",
    "       critical_count = summary['severity_distribution'].get('Critical', 0)\n",
    "       if critical_count > 0:\n",
    "           print(f\"{critical_count} CRITICAL severity attacks detected!\")\n",
    "           \n",
    "           # Show sample critical attacks\n",
    "           critical_samples = df[df['severity'] == 'Critical'].head(5)\n",
    "           if len(critical_samples) > 0:\n",
    "               print(\"\\nSample Critical Attacks:\")\n",
    "               for _, row in critical_samples.iterrows():\n",
    "                   print(f\"  • IP: {row['source_ip']} | URL: {row['request_url'][:50]}...\")\n",
    "                   print(f\"    Attacks: {row['attack_types']}\")\n",
    "       else:\n",
    "           print(\"✓ No critical severity attacks detected\")\n",
    "       \n",
    "       # Top threats\n",
    "       print(\"\\nTOP THREATS:\")\n",
    "       print(\"-\" * 40)\n",
    "       for attack_type, count in list(summary['attack_distribution'].items())[:5]:\n",
    "           print(f\"• {attack_type}: {count:,} instances\")\n",
    "       \n",
    "       # Recommendations\n",
    "       print(\"\\nRECOMMENDATIONS:\")\n",
    "       print(\"-\" * 40)\n",
    "       recommendations = [\n",
    "           \"Deploy Web Application Firewall (WAF)\",\n",
    "           \"Implement rate limiting and geo-blocking\",\n",
    "           \"Enable comprehensive logging and monitoring\",\n",
    "           \"Conduct security code reviews\",\n",
    "           \"Regular penetration testing\"\n",
    "       ]\n",
    "       \n",
    "       for i, rec in enumerate(recommendations, 1):\n",
    "           print(f\"{i}. {rec}\")\n",
    "       \n",
    "       print(\"\\n\" + \"=\"*80)\n",
    "       print(\"ANALYSIS COMPLETE!\")\n",
    "       print(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "       print(\"=\"*80)\n",
    "       \n",
    "       return df, summary\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"\\nERROR: {str(e)}\")\n",
    "       import traceback\n",
    "       traceback.print_exc()\n",
    "       raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize all patterns and functions\n",
    "print(\"All functions loaded successfully!\")\n",
    "print(\"System ready for security analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd13bca",
   "metadata": {},
   "source": [
    "#### Load Data:\n",
    "The dataset is from a honeypot: log_file.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbf71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading honeypot log file...\n",
      "Data loaded successfully!\n",
      "Shape: (720281, 22)\n",
      "Columns: ident, timestamp, unixTimestamp, source_ip, actual_ip, actual_ip_num, source_port, destination_ip, destination_ip_num, destination_port, protocol, country, country_code, Latitude, Longitude, asn_code, asn, filename, User_Agent, request_raw, request_url, X_Forwarded_For\n"
     ]
    }
   ],
   "source": [
    "#### Load Data:\n",
    "# The dataset is from a honeypot: log_file_2.csv\n",
    "\n",
    "# Load the honeypot log file\n",
    "print(\"Loading honeypot log file...\")\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(os.path.join(data_dir, 'log_file_2.csv'), sep='\\t', lineterminator='\\r', dtype='string', encoding='utf8')\n",
    "    \n",
    "    # Remove blank rows\n",
    "    data = data[:-1]\n",
    "    \n",
    "    print(f\"Data loaded successfully!\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Columns: {', '.join(data.columns.tolist())}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: log_file_2.csv not found in current directory\")\n",
    "    print(\"Please ensure the data file is in the same folder as this notebook\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea35d7",
   "metadata": {},
   "source": [
    "#### Data Wrangling:\n",
    "\n",
    "This secion contains the required functions used to clean and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa68da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DATA PREPARATION ---\n",
      "\n",
      "Data Quality Report:\n",
      "  • Total records: 720,281\n",
      "  • Columns: 31\n",
      "  • Missing values in request_url: 0\n",
      "  • Date range: 2016-09-15 07:12:34.475000 to 2019-10-31 13:14:50.017000\n",
      "\n",
      "Sample of cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source_ip</th>\n",
       "      <th>country</th>\n",
       "      <th>request_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-15 07:12:34.475</td>\n",
       "      <td>36.239.154.136</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>/ipc$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-09-15 07:47:46.342</td>\n",
       "      <td>110.87.183.250</td>\n",
       "      <td>China</td>\n",
       "      <td>/ipc$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-09-15 07:51:14.172</td>\n",
       "      <td>191.96.249.80</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>/xmlrpc.php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-09-15 12:01:09.751</td>\n",
       "      <td>203.67.142.246</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>/ipc$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-09-15 12:57:04.966</td>\n",
       "      <td>36.236.95.116</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>/ipc$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp       source_ip         country  request_url\n",
       "0 2016-09-15 07:12:34.475  36.239.154.136          Taiwan        /ipc$\n",
       "1 2016-09-15 07:47:46.342  110.87.183.250           China        /ipc$\n",
       "2 2016-09-15 07:51:14.172   191.96.249.80  United Kingdom  /xmlrpc.php\n",
       "3 2016-09-15 12:01:09.751  203.67.142.246          Taiwan        /ipc$\n",
       "4 2016-09-15 12:57:04.966   36.236.95.116          Taiwan        /ipc$"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "#### Data Wrangling:\n",
    "# This section contains the required functions used to clean and prepare the data\n",
    "\n",
    "# Apply data cleaning\n",
    "print(\"\\n--- DATA PREPARATION ---\")\n",
    "\n",
    "# Initialize data processor\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Clean the data\n",
    "required_columns = ['source_ip', 'request_url', 'timestamp', 'country', 'User_Agent']\n",
    "data = processor.clean_data(data, required_columns)\n",
    "\n",
    "# Enrich data with derived features\n",
    "data = processor.enrich_data(data)\n",
    "\n",
    "# Display data quality metrics\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(f\"  • Total records: {len(data):,}\")\n",
    "print(f\"  • Columns: {data.shape[1]}\")\n",
    "print(f\"  • Missing values in request_url: {data['request_url'].isna().sum()}\")\n",
    "print(f\"  • Date range: {data['timestamp'].min()} to {data['timestamp'].max()}\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(data[['timestamp', 'source_ip', 'country', 'request_url']].head())\n",
    "\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1da27c",
   "metadata": {},
   "source": [
    "#### Search for suspicious activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca707f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SECURITY ANALYSIS ---\n",
      "Using vectorized operations with proper threading...\n",
      "Large dataset detected (720,281 URLs). Using parallel processing...\n",
      "\n",
      "Starting optimized URL security analysis...\n",
      "Processing a large dataset — analysis time will vary based on system resources.\n",
      "\n",
      "Analyzing 720,281 URLs using 4 threads...\n",
      "Created 5 chunks of ~180070 URLs each\n",
      "Analysis completed successfully!\n",
      "\n",
      "Quick Analysis Summary:\n",
      "   Total URLs: 720,281\n",
      "   Suspicious URLs: 664,257 (92.22%)\n",
      "   Multi-attack URLs: 473,728\n",
      "   Average confidence: 0.554\n",
      "\n",
      "Preliminary Top Attack Types:\n",
      "   • PHP Attack: 487,771\n",
      "   • File Inclusion: 9,811\n",
      "   • Directory Traversal: 5,716\n",
      "   • Command Injection: 35,694\n",
      "   • Windows Attack: 220,484\n",
      "\n",
      "--- ANALYSIS RESULTS ---\n",
      "Analysis completed successfully!\n",
      "Total URLs analyzed: 720,281\n",
      "Suspicious URLs found: 664,257 (92.22%)\n",
      "Attack types detected: 10\n",
      "\n",
      "Top Attack Types:\n",
      "   • PHP Attack: 487,771 occurrences\n",
      "   • File Inclusion: 9,811 occurrences\n",
      "   • Directory Traversal: 5,716 occurrences\n",
      "   • Command Injection: 35,694 occurrences\n",
      "   • Windows Attack: 220,484 occurrences\n",
      "\n",
      "Severity Distribution:\n",
      "   • Clean: 56,024 URLs\n",
      "   • Low: 176,088 URLs\n",
      "   • Medium: 18,908 URLs\n",
      "   • Critical: 84,737 URLs\n",
      "   • High: 384,524 URLs\n",
      "\n",
      "Sample Suspicious URLs:\n",
      "\n",
      "   URL: /xmlrpc.php...\n",
      "   Source IP: 191.96.249.80\n",
      "   Attack Types: PHP Attack\n",
      "   Severity: Low (Score: 7.0)\n",
      "   Confidence: 0.300\n",
      "\n",
      "   URL: /phpMyAdmin/scripts/setup.php...\n",
      "   Source IP: 103.22.182.35\n",
      "   Attack Types: PHP Attack\n",
      "   Severity: Low (Score: 7.0)\n",
      "   Confidence: 0.300\n",
      "\n",
      "   URL: /phpMyAdmin/scripts/setup.php...\n",
      "   Source IP: 61.19.80.106\n",
      "   Attack Types: PHP Attack\n",
      "   Severity: Low (Score: 7.0)\n",
      "   Confidence: 0.300\n",
      "\n",
      "   URL: http://httpbin.org/get...\n",
      "   Source IP: 178.33.18.55\n",
      "   Attack Types: File Inclusion\n",
      "   Severity: Medium (Score: 9.0)\n",
      "   Confidence: 0.300\n",
      "\n",
      "   URL: /.git/config...\n",
      "   Source IP: 76.74.178.164\n",
      "   Attack Types: Directory Traversal\n",
      "   Severity: Medium (Score: 8.0)\n",
      "   Confidence: 0.300\n",
      "\n",
      "Security analysis completed successfully!\n",
      "\n",
      "Security analysis completed in 528.55 seconds.\n",
      "Results saved in 'analyzed_data' DataFrame with 37 columns\n"
     ]
    }
   ],
   "source": [
    "#### Search for suspicious activity:\n",
    "# Analyze all URLs using compiled regex patterns to detect various attack types\n",
    "\n",
    "print(\"\\n--- SECURITY ANALYSIS ---\")\n",
    "print(\"Using vectorized operations with proper threading...\")\n",
    "\n",
    "start_time = time.time()  # Start timer\n",
    "\\\n",
    "\n",
    "# 1. Collect regex patterns into dictionary \n",
    "patterns_dict = {\n",
    "    'SQL Injection': OptimizedAttackPatterns.get_sql_injection_pattern(),\n",
    "    'Cross-Site Scripting (XSS)': OptimizedAttackPatterns.get_xss_pattern(),\n",
    "    'PHP Attack': OptimizedAttackPatterns.get_php_attack_pattern(),\n",
    "    'Windows Attack': OptimizedAttackPatterns.get_windows_attack_pattern(),\n",
    "    'Directory Traversal': OptimizedAttackPatterns.get_directory_traversal_pattern(),\n",
    "    'Command Injection': OptimizedAttackPatterns.get_command_injection_pattern(),\n",
    "    'LDAP Injection': OptimizedAttackPatterns.get_ldap_injection_pattern(),\n",
    "    'XML Injection': OptimizedAttackPatterns.get_xml_injection_pattern(),\n",
    "    'NoSQL Injection': OptimizedAttackPatterns.get_nosql_injection_pattern(),\n",
    "    'File Inclusion': OptimizedAttackPatterns.get_file_inclusion_pattern()\n",
    "}\n",
    "\n",
    "# 2. Choose analysis method based on dataset size\n",
    "if len(data) > 10000:\n",
    "    print(f\"Large dataset detected ({len(data):,} URLs). Using parallel processing...\")\n",
    "    try:\n",
    "        # Try parallel processing first\n",
    "        analyzed_data, summary = process_urls_optimized(data, patterns_dict, n_jobs=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Parallel processing failed: {e}\")\n",
    "        print(\"Falling back to single-threaded analysis...\")\n",
    "        analyzed_data = quick_security_analysis(data, patterns_dict)\n",
    "        \n",
    "        # Generate summary manually for fallback\n",
    "        analyzer = VectorizedURLAnalyzer(patterns_dict)\n",
    "        results_list = []\n",
    "        for _, row in analyzed_data.iterrows():\n",
    "            results_list.append({\n",
    "                'is_suspicious': row['is_suspicious'],\n",
    "                'attack_types': row['attack_types'].split(', ') if row['attack_types'] else [],\n",
    "                'attack_count': row['attack_count'],\n",
    "                'severity': row['severity'],\n",
    "                'severity_score': row['severity_score'],\n",
    "                'confidence': row['confidence']\n",
    "            })\n",
    "        summary = analyzer.get_summary_stats(results_list)\n",
    "else:\n",
    "    print(f\"Small dataset ({len(data):,} URLs). Using single-threaded analysis...\")\n",
    "    analyzed_data = quick_security_analysis(data, patterns_dict)\n",
    "    \n",
    "    # Generate summary\n",
    "    analyzer = VectorizedURLAnalyzer(patterns_dict)\n",
    "    results_list = []\n",
    "    for _, row in analyzed_data.iterrows():\n",
    "        results_list.append({\n",
    "            'is_suspicious': row['is_suspicious'],\n",
    "            'attack_types': row['attack_types'].split(', ') if row['attack_types'] else [],\n",
    "            'attack_count': row['attack_count'],\n",
    "            'severity': row['severity'],\n",
    "            'severity_score': row['severity_score'],\n",
    "            'confidence': row['confidence']\n",
    "        })\n",
    "    summary = analyzer.get_summary_stats(results_list)\n",
    "\n",
    "# 3. Display results\n",
    "print(\"\\n--- ANALYSIS RESULTS ---\")\n",
    "print(f\"Analysis completed successfully!\")\n",
    "print(f\"Total URLs analyzed: {len(analyzed_data):,}\")\n",
    "print(f\"Suspicious URLs found: {summary['suspicious_urls']:,} ({summary['attack_rate']:.2f}%)\")\n",
    "print(f\"Attack types detected: {len(summary['attack_distribution'])}\")\n",
    "\n",
    "# Display attack distribution\n",
    "if summary['attack_distribution']:\n",
    "    print(f\"\\nTop Attack Types:\")\n",
    "    for attack_type, count in list(summary['attack_distribution'].items())[:5]:\n",
    "        print(f\"   • {attack_type}: {count:,} occurrences\")\n",
    "\n",
    "# Display severity distribution\n",
    "if summary['severity_distribution']:\n",
    "    print(f\"\\nSeverity Distribution:\")\n",
    "    for severity, count in summary['severity_distribution'].items():\n",
    "        if count > 0:\n",
    "            print(f\"   • {severity}: {count:,} URLs\")\n",
    "\n",
    "# 4. Show sample suspicious URLs if any\n",
    "if summary['suspicious_urls'] > 0:\n",
    "    print(f\"\\nSample Suspicious URLs:\")\n",
    "    suspicious_sample = analyzed_data[analyzed_data['is_suspicious'] == True].head(5)\n",
    "    \n",
    "    for idx, row in suspicious_sample.iterrows():\n",
    "        print(f\"\\n   URL: {row['request_url'][:80]}...\")\n",
    "        print(f\"   Source IP: {row['source_ip']}\")\n",
    "        print(f\"   Attack Types: {row['attack_types']}\")\n",
    "        print(f\"   Severity: {row['severity']} (Score: {row['severity_score']})\")\n",
    "        print(f\"   Confidence: {row['confidence']:.3f}\")\n",
    "\n",
    "end_time = time.time()  # End timer\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "print(f\"\\nSecurity analysis completed successfully!\")\n",
    "print(f\"\\nSecurity analysis completed in {elapsed:.2f} seconds.\")\n",
    "print(f\"Results saved in 'analyzed_data' DataFrame with {len(analyzed_data.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e6c59",
   "metadata": {},
   "source": [
    "#### Save the results for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d899e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAVING RESULTS ---\n",
      "HTML report saved: /Users/versa_kyle/Downloads/GitHub Portfolio Projects/Cybersecurity Projects/URL_Threat_Detection_Analysis/Reports/security_report_20260214_134339.html\n",
      "Created 4 visualizations\n",
      "Writing output files to disk... this may take a minute.\n",
      "\n",
      "\n",
      "Exported files:\n",
      "  • full_csv: /Users/versa_kyle/Downloads/GitHub Portfolio Projects/Cybersecurity Projects/URL_Threat_Detection_Analysis/Reports/full_analysis_20260214_134339.csv\n",
      "  • suspicious_csv: /Users/versa_kyle/Downloads/GitHub Portfolio Projects/Cybersecurity Projects/URL_Threat_Detection_Analysis/Reports/suspicious_only_20260214_134339.csv\n",
      "  • critical_csv: /Users/versa_kyle/Downloads/GitHub Portfolio Projects/Cybersecurity Projects/URL_Threat_Detection_Analysis/Reports/critical_attacks_20260214_134339.csv\n",
      "  • summary_json: /Users/versa_kyle/Downloads/GitHub Portfolio Projects/Cybersecurity Projects/URL_Threat_Detection_Analysis/Reports/summary_20260214_134339.json\n",
      "  • executive_summary: /Users/versa_kyle/Downloads/GitHub Portfolio Projects/Cybersecurity Projects/URL_Threat_Detection_Analysis/Reports/executive_summary_20260214_134339.txt\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n",
      "Total suspicious activity detected: 664,257 URLs\n",
      "Critical threats: 84737\n",
      "Unique attack patterns: 10\n",
      "\n",
      "All results have been saved to: /Users/versa_kyle/Downloads/GitHub Portfolio Projects/Cybersecurity Projects/URL_Threat_Detection_Analysis/Reports\n"
     ]
    }
   ],
   "source": [
    "#### Save the results for further analysis:\n",
    "# Generate comprehensive reports, visualizations, and export data in multiple formats\n",
    "\n",
    "# Note: Analysis of 851,321 URLs typically takes 10-15 minutes depending on system resources.\n",
    "# This is comparable to commercial security tools performing similar comprehensive pattern matching.\n",
    "# The file export operations add an additional 2-3 minutes for writing large CSV files.\n",
    "\n",
    "print(\"\\n--- SAVING RESULTS ---\")\n",
    "\n",
    "# Initialize report generator\n",
    "generator = ReportGenerator(output_dir)\n",
    "\n",
    "# Generate comprehensive HTML report\n",
    "html_report = generator.generate_html_report(analyzed_data, summary)\n",
    "print(f\"HTML report saved: {html_report}\")\n",
    "\n",
    "# Create visualizations\n",
    "visualizations = generator.create_visualizations(analyzed_data, summary)\n",
    "print(f\"Created {len(visualizations)} visualizations\")\n",
    "\n",
    "print(\"Writing output files to disk... this may take a minute.\\n\")\n",
    "\n",
    "# Export data files\n",
    "export_paths = generator.export_results(analyzed_data, summary)\n",
    "print(\"\\nExported files:\")\n",
    "for file_type, path in export_paths.items():\n",
    "    print(f\"  • {file_type}: {path}\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total suspicious activity detected: {summary['suspicious_urls']:,} URLs\")\n",
    "print(f\"Critical threats: {summary['severity_distribution'].get('Critical', 0)}\")\n",
    "\n",
    "# Fix for the unique_attack_patterns reference\n",
    "if 'top_attack_combinations' in summary:\n",
    "    print(f\"Unique attack patterns: {len(summary['top_attack_combinations'])}\")\n",
    "else:\n",
    "    print(f\"Unique attack patterns: {len(summary['attack_distribution'])}\")\n",
    "\n",
    "print(\"\\nAll results have been saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77580fd",
   "metadata": {},
   "source": [
    "## Observations, Analysis, and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e62a5476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED OBSERVATIONS AND ANALYSIS\n",
      "================================================================================\n",
      "\n",
      " EXECUTIVE SUMMARY:\n",
      "----------------------------------------\n",
      "Dataset: Honeypot log containing 720,281 HTTP requests\n",
      "Analysis Period: 2016-09-15 to 2019-10-31\n",
      "Overall Threat Level: CRITICAL - 92.2% malicious traffic detected\n",
      "Unique Attack Patterns: 10 different combinations identified\n",
      "\n",
      "1. ATTACK PATTERN ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "Most Common Attack Combinations:\n",
      "  - SQL Injection, PHP Attack: 228,536 occurrences (34.40% of all attacks)\n",
      "  - SQL Injection, Windows Attack: 141,494 occurrences (21.30% of all attacks)\n",
      "  - SQL Injection, PHP Attack, Windows Attack: 45,334 occurrences (6.82% of all attacks)\n",
      "  - SQL Injection, PHP Attack, Command Injection: 15,288 occurrences (2.30% of all attacks)\n",
      "  - SQL Injection, Windows Attack, Command Injection: 7,235 occurrences (1.09% of all attacks)\n",
      "  - PHP Attack, Command Injection: 6,845 occurrences (1.03% of all attacks)\n",
      "  - SQL Injection, PHP Attack, Windows Attack, NoSQL Injection: 5,592 occurrences (0.84% of all attacks)\n",
      "  - PHP Attack, File Inclusion: 5,016 occurrences (0.76% of all attacks)\n",
      "  - PHP Attack, Windows Attack: 4,382 occurrences (0.66% of all attacks)\n",
      "  - SQL Injection, PHP Attack, Directory Traversal: 3,214 occurrences (0.48% of all attacks)\n",
      "\n",
      "Attack Complexity Statistics:\n",
      "  - Mean attacks per malicious URL: 1.86\n",
      "  - Median attacks per malicious URL: 2\n",
      "  - Maximum attack types in single URL: 6.0\n",
      "\n",
      "2. TEMPORAL ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "Attack Timing Patterns:\n",
      "  - Peak attack hour: 7:00 with 43,307 attacks\n",
      "  - Lowest attack hour: 23:00 with 17,529 attacks\n",
      "  - Attack variance by hour: 7554.08\n",
      "\n",
      "  - Most active day: Wednesday (180,433 attacks)\n",
      "  - Least active day: Tuesday (49,850 attacks)\n",
      "\n",
      "  - Average attack velocity: 24.3 attacks/hour\n",
      "\n",
      "3. GEOGRAPHIC ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "Geographic Distribution:\n",
      "  - Attacks originated from 147 different countries\n",
      "\n",
      "  Top 10 Attacking Countries:\n",
      "    United States: 241,462 attacks (36.35%)\n",
      "    China: 49,812 attacks (7.50%)\n",
      "    United Kingdom: 43,125 attacks (6.49%)\n",
      "    Russia: 41,162 attacks (6.20%)\n",
      "    Brazil: 27,703 attacks (4.17%)\n",
      "    Iraq: 25,954 attacks (3.91%)\n",
      "    Netherlands: 22,676 attacks (3.41%)\n",
      "    Indonesia: 15,048 attacks (2.27%)\n",
      "    Ukraine: 12,242 attacks (1.84%)\n",
      "    Thailand: 11,508 attacks (1.73%)\n",
      "\n",
      "  Countries with Most Sophisticated Attacks (3+ attack types):\n",
      "    United States: 33,793 multi-vector attacks (14.0% sophistication rate)\n",
      "    Brazil: 6,275 multi-vector attacks (22.7% sophistication rate)\n",
      "    United Kingdom: 5,192 multi-vector attacks (12.0% sophistication rate)\n",
      "    Netherlands: 3,874 multi-vector attacks (17.1% sophistication rate)\n",
      "    Ukraine: 3,332 multi-vector attacks (27.2% sophistication rate)\n",
      "\n",
      "4. TARGET ANALYSIS:\n",
      "----------------------------------------\n",
      "\n",
      "Most Targeted Resources:\n",
      "\n",
      "  Target Categories:\n",
      "    - Administrative interfaces: 1 paths\n",
      "    - PHP applications: 10 paths\n",
      "    - Configuration files: 0 paths\n",
      "\n",
      "  Top 10 Specific Targets:\n",
      "    1. /shopdb/noticia.php: 10,762 attacks\n",
      "    2. /cgi-bin/enc/control/affiliate-agreement.cfm: 9,351 attacks\n",
      "    3. /user/base.php: 8,120 attacks\n",
      "    4. /phpmy/offer.php: 7,628 attacks\n",
      "    5. /phpmyadmin/index.php: 6,511 attacks\n",
      "    6. /cgi-bin/enc/inc/header.php/step_one.php: 6,333 attacks\n",
      "    7. /scripts/base.php: 5,704 attacks\n",
      "    8. /phpmy/wwwroot/catalog.asp: 5,321 attacks\n",
      "    9. /embed/blank.php: 5,300 attacks\n",
      "    10. /shopdb/al_initialize.php: 5,085 attacks\n",
      "\n",
      "5. ATTACK SOPHISTICATION METRICS:\n",
      "----------------------------------------\n",
      "\n",
      "Attack Complexity Distribution:\n",
      "  - 1.0 attack type(s): 190,529 URLs (28.68%)\n",
      "  - 2.0 attack type(s): 389,649 URLs (58.66%)\n",
      "  - 3.0 attack type(s): 73,605 URLs (11.08%)\n",
      "  - 4.0 attack type(s): 10,325 URLs (1.55%)\n",
      "  - 5.0 attack type(s): 142 URLs (0.02%)\n",
      "  - 6.0 attack type(s): 7 URLs (0.00%)\n",
      "\n",
      "  Severity Correlation:\n",
      "    - High/Critical severity attacks: 469,261\n",
      "    - Multi-vector high severity: 469,261 (100.0%)\n",
      "\n",
      "6. ATTACK EFFECTIVENESS INDICATORS:\n",
      "----------------------------------------\n",
      "\n",
      "Detection Confidence:\n",
      "  - High confidence detections (≥80%): 84,079 (12.7%)\n",
      "  - Average detection confidence: 55.4%\n",
      "\n",
      "  Attack Persistence:\n",
      "    - IPs with >100 attacks: 1109\n",
      "    - Most persistent attacker: 166.62.85.161 (25,295 attacks)\n",
      "\n",
      "7. CONCLUSIONS:\n",
      "----------------------------------------\n",
      "\n",
      "KEY FINDINGS:\n",
      "1. CRITICAL THREAT LEVEL: 92.2% of all traffic is malicious, indicating active targeting\n",
      "2. HIGH SOPHISTICATION: 71.3% of attacks use multiple vectors, suggesting advanced threat actors\n",
      "3. PRIMARY THREATS: PHP Attack (487,771), File Inclusion (9,811), Directory Traversal (5,716)\n",
      "4. GLOBAL THREAT: Attacks from 147 countries indicate widespread interest\n",
      "\n",
      "8. SECURITY RECOMMENDATIONS:\n",
      "----------------------------------------\n",
      "\n",
      "IMMEDIATE ACTIONS (0-7 days):\n",
      "  1. Deploy Web Application Firewall (WAF) with rules targeting identified attack patterns\n",
      "  2. Implement IP-based rate limiting (suggested: 100 requests/minute per IP)\n",
      "  3. Block or monitor top 100 attacking IPs identified in this analysis\n",
      "\n",
      "SHORT-TERM ACTIONS (1-4 weeks):\n",
      "  1. Audit all database queries and implement prepared statements\n",
      "  2. Review PHP configuration and disable dangerous functions (eval, exec, system)\n",
      "  3. Implement path validation and chroot jails for file operations\n",
      "\n",
      "LONG-TERM ACTIONS (1-3 months):\n",
      "  1. Implement comprehensive Security Information and Event Management (SIEM)\n",
      "  2. Establish 24/7 security monitoring and incident response team\n",
      "  3. Develop and test incident response playbooks for each attack type\n",
      "  4. Conduct quarterly penetration testing and security assessments\n",
      "\n",
      "9. STUDY LIMITATIONS AND FUTURE WORK:\n",
      "----------------------------------------\n",
      "\n",
      "Limitations:\n",
      "  - Analysis based on honeypot data may not reflect all real-world attack patterns\n",
      "  - Geographic attribution based on IP addresses may be inaccurate due to VPNs/proxies\n",
      "  - Some sophisticated attacks may evade regex-based detection\n",
      "\n",
      "Future Work:\n",
      "  - Implement machine learning models for anomaly detection\n",
      "  - Correlate attack patterns with threat intelligence feeds\n",
      "  - Analyze payload contents for malware signatures\n",
      "  - Develop predictive models for attack forecasting\n",
      "  - Optimize regex patterns for improved performance on large datasets\n",
      "  - Implement real-time streaming analysis for live traffic\n",
      "\n",
      "================================================================================\n",
      "END OF SECURITY ANALYSIS REPORT\n",
      "Report Generated: 2026-02-14 13:45:48\n",
      "Analyst: Kyle Purves\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Observations, Analysis, and Conclusions\n",
    "\n",
    "# Detailed analysis of findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED OBSERVATIONS AND ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Executive Summary\n",
    "print(\"\\n EXECUTIVE SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Dataset: Honeypot log containing {len(analyzed_data):,} HTTP requests\")\n",
    "print(f\"Analysis Period: {analyzed_data['timestamp'].min().strftime('%Y-%m-%d')} to {analyzed_data['timestamp'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Overall Threat Level: CRITICAL - {(summary['suspicious_urls']/summary['total_urls']*100):.1f}% malicious traffic detected\")\n",
    "print(f\"Unique Attack Patterns: {len(summary['top_attack_combinations'])} different combinations identified\")\n",
    "\n",
    "# 1. Attack Pattern Analysis\n",
    "print(\"\\n1. ATTACK PATTERN ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Most common attack combinations\n",
    "if analyzed_data['attack_count'].max() > 1:\n",
    "    multi_attacks = analyzed_data[analyzed_data['attack_count'] > 1]['attack_types'].value_counts().head(10)\n",
    "    print(\"\\nMost Common Attack Combinations:\")\n",
    "    for combo, count in multi_attacks.items():\n",
    "        percentage = (count / summary['suspicious_urls']) * 100\n",
    "        print(f\"  - {combo}: {count:,} occurrences ({percentage:.2f}% of all attacks)\")\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\nAttack Complexity Statistics:\")\n",
    "    print(f\"  - Mean attacks per malicious URL: {analyzed_data[analyzed_data['is_suspicious']]['attack_count'].mean():.2f}\")\n",
    "    print(f\"  - Median attacks per malicious URL: {analyzed_data[analyzed_data['is_suspicious']]['attack_count'].median():.0f}\")\n",
    "    print(f\"  - Maximum attack types in single URL: {analyzed_data['attack_count'].max()}\")\n",
    "\n",
    "# 2. Temporal Analysis\n",
    "print(\"\\n2. TEMPORAL ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'timestamp' in analyzed_data.columns and analyzed_data['timestamp'].notna().any():\n",
    "    # Optimized temporal analysis\n",
    "    hourly_attacks = analyzed_data[analyzed_data['is_suspicious']].groupby('hour').size()\n",
    "    \n",
    "    if len(hourly_attacks) > 0:\n",
    "        peak_hour = hourly_attacks.idxmax()\n",
    "        low_hour = hourly_attacks.idxmin()\n",
    "        \n",
    "        print(f\"\\nAttack Timing Patterns:\")\n",
    "        print(f\"  - Peak attack hour: {peak_hour}:00 with {hourly_attacks[peak_hour]:,} attacks\")\n",
    "        print(f\"  - Lowest attack hour: {low_hour}:00 with {hourly_attacks[low_hour]:,} attacks\")\n",
    "        print(f\"  - Attack variance by hour: {hourly_attacks.std():.2f}\")\n",
    "        \n",
    "        # Day of week analysis\n",
    "        analyzed_data['day_name'] = analyzed_data['timestamp'].dt.day_name()\n",
    "        daily_pattern = analyzed_data[analyzed_data['is_suspicious']].groupby('day_name').size()\n",
    "        print(f\"\\n  - Most active day: {daily_pattern.idxmax()} ({daily_pattern.max():,} attacks)\")\n",
    "        print(f\"  - Least active day: {daily_pattern.idxmin()} ({daily_pattern.min():,} attacks)\")\n",
    "        \n",
    "        # Attack speed\n",
    "        time_span = (analyzed_data['timestamp'].max() - analyzed_data['timestamp'].min()).total_seconds() / 3600\n",
    "        attacks_per_hour = summary['suspicious_urls'] / time_span\n",
    "        print(f\"\\n  - Average attack velocity: {attacks_per_hour:.1f} attacks/hour\")\n",
    "\n",
    "# 3. Geographic Analysis\n",
    "print(\"\\n3. GEOGRAPHIC ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'country' in analyzed_data.columns:\n",
    "    # Geographic diversity\n",
    "    unique_countries = analyzed_data[analyzed_data['is_suspicious']]['country'].nunique()\n",
    "    print(f\"\\nGeographic Distribution:\")\n",
    "    print(f\"  - Attacks originated from {unique_countries} different countries\")\n",
    "    \n",
    "    # Top attacking countries with percentages\n",
    "    country_attacks = analyzed_data[analyzed_data['is_suspicious']]['country'].value_counts().head(10)\n",
    "    print(f\"\\n  Top 10 Attacking Countries:\")\n",
    "    for country, count in country_attacks.items():\n",
    "        percentage = (count / summary['suspicious_urls']) * 100\n",
    "        print(f\"    {country}: {count:,} attacks ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Countries with highest attack sophistication\n",
    "    sophisticated_attacks = analyzed_data[analyzed_data['attack_count'] > 2]\n",
    "    if len(sophisticated_attacks) > 0:\n",
    "        sophisticated_countries = sophisticated_attacks['country'].value_counts().head(5)\n",
    "        print(\"\\n  Countries with Most Sophisticated Attacks (3+ attack types):\")\n",
    "        for country, count in sophisticated_countries.items():\n",
    "            sophistication_rate = (count / country_attacks.get(country, count)) * 100\n",
    "            print(f\"    {country}: {count:,} multi-vector attacks ({sophistication_rate:.1f}% sophistication rate)\")\n",
    "\n",
    "# 4. Target Analysis\n",
    "print(\"\\n4. TARGET ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Most targeted paths with categorization\n",
    "targeted_paths = analyzed_data[analyzed_data['is_suspicious']]['request_url'].str.split('?').str[0]\n",
    "path_counts = targeted_paths.value_counts().head(15)\n",
    "\n",
    "print(\"\\nMost Targeted Resources:\")\n",
    "# Categorize paths\n",
    "admin_paths = [p for p in path_counts.index if 'admin' in p.lower() or 'login' in p.lower()]\n",
    "php_paths = [p for p in path_counts.index if '.php' in p]\n",
    "config_paths = [p for p in path_counts.index if 'config' in p.lower() or '.xml' in p or '.ini' in p]\n",
    "\n",
    "print(f\"\\n  Target Categories:\")\n",
    "print(f\"    - Administrative interfaces: {len(admin_paths)} paths\")\n",
    "print(f\"    - PHP applications: {len(php_paths)} paths\")\n",
    "print(f\"    - Configuration files: {len(config_paths)} paths\")\n",
    "\n",
    "print(\"\\n  Top 10 Specific Targets:\")\n",
    "for i, (path, count) in enumerate(path_counts.head(10).items(), 1):\n",
    "    print(f\"    {i}. {path}: {count:,} attacks\")\n",
    "\n",
    "# 5. Attack Sophistication Metrics\n",
    "print(\"\\n5. ATTACK SOPHISTICATION METRICS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "total_suspicious = analyzed_data['is_suspicious'].sum()\n",
    "if total_suspicious > 0:\n",
    "    # Detailed breakdown\n",
    "    attack_distribution = analyzed_data[analyzed_data['is_suspicious']]['attack_count'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nAttack Complexity Distribution:\")\n",
    "    for num_attacks, count in attack_distribution.items():\n",
    "        percentage = (count / total_suspicious) * 100\n",
    "        print(f\"  - {num_attacks} attack type(s): {count:,} URLs ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Severity connection\n",
    "    high_severity = analyzed_data[analyzed_data['severity'].isin(['Critical', 'High'])].shape[0]\n",
    "    multi_vector_high = analyzed_data[(analyzed_data['attack_count'] > 1) & \n",
    "                                      (analyzed_data['severity'].isin(['Critical', 'High']))].shape[0]\n",
    "    \n",
    "    print(f\"\\n  Severity Correlation:\")\n",
    "    print(f\"    - High/Critical severity attacks: {high_severity:,}\")\n",
    "    print(f\"    - Multi-vector high severity: {multi_vector_high:,} ({multi_vector_high/high_severity*100:.1f}%)\")\n",
    "\n",
    "# 6. Attack Effectiveness Analysis\n",
    "print(\"\\n6. ATTACK EFFECTIVENESS INDICATORS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Confidence analysis\n",
    "high_confidence = analyzed_data[analyzed_data['confidence'] >= 0.8].shape[0]\n",
    "print(f\"\\nDetection Confidence:\")\n",
    "print(f\"  - High confidence detections (≥80%): {high_confidence:,} ({high_confidence/total_suspicious*100:.1f}%)\")\n",
    "print(f\"  - Average detection confidence: {analyzed_data[analyzed_data['is_suspicious']]['confidence'].mean()*100:.1f}%\")\n",
    "\n",
    "# Attack persistence \n",
    "if 'source_ip' in analyzed_data.columns:\n",
    "    ip_counts = analyzed_data[analyzed_data['is_suspicious']]['source_ip'].value_counts()\n",
    "    persistent_ips = ip_counts[ip_counts > 100].shape[0]\n",
    "    print(f\"\\n  Attack Persistence:\")\n",
    "    print(f\"    - IPs with >100 attacks: {persistent_ips}\")\n",
    "    print(f\"    - Most persistent attacker: {ip_counts.index[0]} ({ip_counts.iloc[0]:,} attacks)\")\n",
    "\n",
    "# 7. Conclusions\n",
    "print(\"\\n7. CONCLUSIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "conclusions = []\n",
    "\n",
    "# Overall threat assessment\n",
    "threat_percentage = (summary['suspicious_urls'] / summary['total_urls']) * 100\n",
    "conclusions.append(f\"CRITICAL THREAT LEVEL: {threat_percentage:.1f}% of all traffic is malicious, indicating active targeting\")\n",
    "\n",
    "# Attack sophistication\n",
    "sophistication_rate = (summary['multi_attack_urls'] / summary['suspicious_urls']) * 100\n",
    "conclusions.append(f\"HIGH SOPHISTICATION: {sophistication_rate:.1f}% of attacks use multiple vectors, suggesting advanced threat actors\")\n",
    "\n",
    "# Primary threats\n",
    "top_3_attacks = list(summary['attack_distribution'].items())[:3]\n",
    "threat_summary = \", \".join([f\"{attack[0]} ({attack[1]:,})\" for attack in top_3_attacks])\n",
    "conclusions.append(f\"PRIMARY THREATS: {threat_summary}\")\n",
    "\n",
    "# Geographic insights\n",
    "if unique_countries > 50:\n",
    "    conclusions.append(f\"GLOBAL THREAT: Attacks from {unique_countries} countries indicate widespread interest\")\n",
    "\n",
    "# Temporal insights\n",
    "if attacks_per_hour > 1000:\n",
    "    conclusions.append(f\"SUSTAINED CAMPAIGN: {attacks_per_hour:.0f} attacks/hour suggests automated, persistent threats\")\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "for i, conclusion in enumerate(conclusions, 1):\n",
    "    print(f\"{i}. {conclusion}\")\n",
    "\n",
    "# 8. Recommendations\n",
    "print(\"\\n8. SECURITY RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nIMMEDIATE ACTIONS (0-7 days):\")\n",
    "immediate = [\n",
    "    \"Deploy Web Application Firewall (WAF) with rules targeting identified attack patterns\",\n",
    "    \"Implement IP-based rate limiting (suggested: 100 requests/minute per IP)\",\n",
    "    \"Block or monitor top 100 attacking IPs identified in this analysis\"\n",
    "]\n",
    "for i, rec in enumerate(immediate, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\nSHORT-TERM ACTIONS (1-4 weeks):\")\n",
    "short_term = []\n",
    "if 'SQL Injection' in summary['attack_distribution']:\n",
    "    short_term.append(\"Audit all database queries and implement prepared statements\")\n",
    "if 'PHP Attack' in summary['attack_distribution']:\n",
    "    short_term.append(\"Review PHP configuration and disable dangerous functions (eval, exec, system)\")\n",
    "if 'Directory Traversal' in summary['attack_distribution']:\n",
    "    short_term.append(\"Implement path validation and chroot jails for file operations\")\n",
    "\n",
    "for i, rec in enumerate(short_term, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\nLONG-TERM ACTIONS (1-3 months):\")\n",
    "long_term = [\n",
    "    \"Implement comprehensive Security Information and Event Management (SIEM)\",\n",
    "    \"Establish 24/7 security monitoring and incident response team\",\n",
    "    \"Develop and test incident response playbooks for each attack type\",\n",
    "    \"Conduct quarterly penetration testing and security assessments\"\n",
    "]\n",
    "for i, rec in enumerate(long_term, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "# 9. Limitations and Future Work\n",
    "print(\"\\n9. STUDY LIMITATIONS AND FUTURE WORK:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"  - Analysis based on honeypot data may not reflect all real-world attack patterns\")\n",
    "print(\"  - Geographic attribution based on IP addresses may be inaccurate due to VPNs/proxies\")\n",
    "print(\"  - Some sophisticated attacks may evade regex-based detection\")\n",
    "\n",
    "print(\"\\nFuture Work:\")\n",
    "print(\"  - Implement machine learning models for anomaly detection\")\n",
    "print(\"  - Correlate attack patterns with threat intelligence feeds\")\n",
    "print(\"  - Analyze payload contents for malware signatures\")\n",
    "print(\"  - Develop predictive models for attack forecasting\")\n",
    "print(\"  - Optimize regex patterns for improved performance on large datasets\")\n",
    "print(\"  - Implement real-time streaming analysis for live traffic\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF SECURITY ANALYSIS REPORT\")\n",
    "print(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Analyst: Kyle Purves\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290581e6-63dd-4a49-8e3a-381919fbbb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
